["Citation: Schulte-Tigges, J.; F\u00f6rster,\nM.; Nikolovski, G.; Reke, M.; Ferrein,\nA.; Kaszner, D.; Matheis, D.; Walter, T.\nBenchmarking of Various LiDAR\nSensors for Use in Self-Driving\nVehicles in Real-World Environments.\nSensors 2022 ,22, 7146. https://\ndoi.org/10.3390/s22197146\nAcademic Editor: Boris Miller\nReceived: 18 August 2022\nAccepted: 14 September 2022\nPublished: 21 September 2022\nPublisher\u2019s Note: MDPI stays neutral\nwith regard to jurisdictional claims in\npublished maps and institutional af\ufb01l-\niations.\nCopyright: \u00a9 2022 by the authors.\nLicensee MDPI, Basel, Switzerland.\nThis article is an open access article\ndistributed under the terms and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nsensors\nArticle\nBenchmarking of Various LiDAR Sensors for Use in\nSelf-Driving Vehicles in Real-World Environments\nJoschua Schulte-Tigges1,*\n, Marco F\u00f6rster1, Gjorgji Nikolovski1\n, Michael Reke1,*\n, Alexander Ferrein1\n,\nDaniel Kaszner", "2\n, Dominik Matheis2\nand Thomas Walter2\n1Mobile Autonomous Systems and Cognitive Robotics Institute, FH Aachen\u2014Aachen University of Applied\nSciences, 52066 Aachen, Germany\n2Hyundai Motor Europe Technical Center GmbH, 65428 R\u00fcsselsheim am Main, Germany\n*Correspondence: schulte-tigges@fh-aachen.de (J.S.-T.); reke@fh-aachen.de (M.R.)\nAbstract: In this paper, we report on our benchmark results of the LiDAR sensors Livox Horizon,\nRobosense M1, Blickfeld Cube, Blickfeld Cube Range, Velodyne Velarray H800, and Innoviz Pro.\nThe idea was to test the sensors in different typical scenarios that were de\ufb01ned with real-world use\ncases in mind, in order to \ufb01nd a sensor that meet the requirements of self-driving vehicles. For this,\nwe de\ufb01ned static and dynamic benchmark scenarios. In the static scenarios, both LiDAR and the\ndetection target do not move during the measurement. In dynamic scenarios, the LiDAR sensor was\nmounted on the vehicle which was driving toward the detection target. We tested all ", "mentioned\nLiDAR sensors in both scenarios, show the results regarding the detection accuracy of the targets,\nand discuss their usefulness for deployment in self-driving cars.\nKeywords: LiDAR; benchmark; self-driving\n1. Introduction\nWhile \ufb01rst car manufacturers are receiving approvals for SAE level 3 self-driving func-\ntions (see [ 1,2]), the whole industry is making huge progress to enter the era of automated\ndriving. A self-driving car needs to perceive its environment through its sensors, interpret\nthe sensed data, plan and decide future actions to take and \ufb01nally perform the chosen ac-\ntions. Despite some prominent opinions that the only required sensor type are vision-based\nsensors, many manufacturers see the need for the use of further sensor technologies in order\nto increase the accuracy and reliability of the vehicle-surrounding perception required for\nhigher levels of automation. They investigate other sensors, such as RADAR or LiDAR [ 3],\nfor perceiving the environment of the ", "self-driving car [ 4]. Additionally, LiDAR sensors\nhave proven their usefulness in academic self-driving projects, such as the DARPA Grand\nChallenge [5], which raises vehicle OEMs interest for use in series development as well.\nIn this paper, a benchmark of currently available non-rotary LiDAR sensors is pre-\nsented to identify suitable LiDAR hardware for real-world usage. The selected LiDARs\nwere tested in our automated driving (AD) platform, which was developed in a long-\nstanding research cooperation between the Hyundai Motor Europe Technical Center GmbH\n(HMETC) and the University of Applied Science (FH) Aachen. The overall goal is to\ndevelop and to integrate our automated driving software framework [ 6] into a HMETC\nprototype vehicle to support the European research activities of HMETC, particularly in\nthe European project Hi-Drive [7].\nOne common argument against LiDAR sensors for mass-produced cars is their price,\nwhich usually heavily exceeds the price of vision-based systems. C", "urrent technical devel-\nopments, such as the development of solid-state LiDARs, have further reduced the price of\nthese systems, making them more attractive for vehicle OEM series development activities.\nThe systems presented in this paper, for instance, start at around USD 1300. We present\nand evaluate four different test scenarios, which can be translated into real-world use cases:\nSensors 2022 ,22, 7146. https://doi.org/10.3390/s22197146 https://www.mdpi.com/journal/sensors Sensors 2022 ,22, 7146 2 of 20\nScenario 1: Detecting sphere targets in increasing distances;\nScenario 2: Detecting a square meter reference plane;\nScenario 3: Detecting sphere targets in motion;\nScenario 4: Detecting other vehicles in motion.\nBased on the use case, the test scenario results can be weighted differently: For instance,\nwhen performing a slow manoeuver in a parking space, a sensor which is good at a close\ndistance with a high accuracy is required, while for highway driving, it is more important\nto ha", "ve a high range; as for the accuracy of the sensor in this scenario, accuracies exceeding\n1 cm is acceptable, while higher accuracies might be required on a car park. A more detailed\ndescription of the different scenarios is given in Section 3.\nOur results show that the different sensors perform quite differently in the various test\nscenarios. Some perform well for static scenarios, while they fail at the dynamic scenarios;\nsome have high deviations in measuring depth information while others struggle with\nprecise measurements for xand yranges. We further investigated the observed phenomena,\nand our \ufb01ndings show that this also depends on the scanning patterns that the different\nsensors use. Other observations showed that some sensors have particular problems on\nobject edges, such as crash or jersey barriers. The main contributions of this paper are\nas follows:\n1. To propose a LiDAR benchmark in realistic drive scenarios.\n2. To \ufb01nd particular relationships between the scan patterns and ", "the performance in the\nproposed real-word tests.\n3. To come up with a list of state-of-the-art scanning devices for self-driving cars.\nThe paper is organized as follows: In Section 2, we discuss related research, while in\nSection 3, we de\ufb01ne four different benchmarks, present the results and discuss them in\nSections 4 and 5, respectively. We conclude with Section 6.\n2. Related Work\nRecently, LiDAR sensors have gained more attention from the scienti\ufb01c community,\nas many research results about perception [ 8] and localisation [ 9,10] algorithms have\nbeen published.\nThe team of Lambert, Carballo et al. [ 11] followed a similar approach and shared in\ntheir work similar ideas. They analyzed 10 different LiDARs. Similar to our work, they\ndesigned their tests with multiple detection targets and varying distance as well as the\nsurface re\ufb02ectivity of the objects being measured. Furthermore, they used a self-calibration\nmethod for each LiDAR. However, their research focuses solely on static scen", "arios, whereas\nwe also study how the tested LiDAR sensors perform when the vehicle is in motion passing\na still-standing target better re\ufb02ecting real-world situations. By doing so, one of our \ufb01ndings\nshows that the sensors should be calibrated differently when deploying them in static or\ndynamic use cases. We discuss this issue in depth in Section 5. This benchmark is focused\non solid-state LiDARs instead of rotary 360\u00b0 LiDARs.\nIn contrast to our approach with designing tests suited for real-world scenarios, Cattini,\nCassaneli et al. [ 12] proposed a different test setup. They introduced a procedure to create\na very repeatable and precise testing setup for LiDAR sensors. It constraints multiple\nmotions of displacement and rotation of the sensor and the measurement targets to a single\ndimension, which allows for a more precise setup of the tests. It also incorporates camera\ndata for the evaluation. While the precision and repeatability is important, the scalability\nand variability of su", "ch constrained testing setups is, however, very limited.\nBijelic, Gruber, Ritter et al. benchmarked LiDARs in a fog chamber [ 13]. Their goal\nwas to evaluate the performance of LiDARs when being exposed to varying amounts of\nfog. They tested four sensors from two major manufacturers. Their evaluation was based\non comparing the birds-eye-view representation of point clouds and the intensities of parts\nof point clouds recorded from a \ufb01xed position at different exposure levels of fog.\nThe International Society of Optics and Photonics has also drafted a set of tests and\nspeci\ufb01cations on how to execute and evaluate the tests [ 14]. The draft recommends testing Sensors 2022 ,22, 7146 3 of 20\nLiDARs on functions with high security implications, such as the detection of children-sized\ntargets or in\ufb02uence on human eyesight when exposed to LiDARs.\nAs shown in [ 8], in particular, the \ufb01eld of deep learning for 3D object detection based\non LiDAR data has made major leaps forward by yielding higher", " accuracies than previ-\nous methods. Some of the best performing algorithms in terms of quality and execution\ntimes are algorithms processing only point cloud information, which often are provided\nby a LiDAR. Good examples for this are PV-RCNN [ 15], PointPillars [ 16] and PointR-\nCNN [ 17], to name just a few. Despite the mentioned approaches use different methods,\nthey have in common that they all extract features from the point cloud: (1) PC-RCNN\nuses a combination of voxel- and point-based feature extractions; (2) PointPillar deploys a\nfeature extraction from pillar-like groupings of points in a pseudo-image representation of\na point cloud; and (3) relies solely on a point-based feature extraction. The three networks\nshow signi\ufb01cantly different performances when faced with different properties in the\nunderlying point cloud data [ 18]. Properties in\ufb02uencing the performance outcome include\nnoisiness and density. Both are properties de\ufb01ned by a selected sensor for testing and\nenvironm", "ental in\ufb02uences.\nFor the \ufb01eld of perception-based localization, Qin Zou, Qin Sun, Long Chen et al. [ 10]\nshowed a good comparison of many of the bleeding-edge algorithms in said \ufb01eld. The al-\ngorithms utilize such methods as Kalman \ufb01lters, particle \ufb01lters and loop closure to achieve\nthe goal of localizing an agent traversing unknown terrain, while also creating maps of\nthe surroundings.\nFor the veri\ufb01cation and presentation of the performance, researchers usually test their\nalgorithms on publicly available LiDAR-based point-cloud data sets. Benchmarking data\nsets, such as Kitti [ 19], NuScenes [ 20] and Waymo [ 21], are currently very popular in this\nregard. Each provides a large number of (labeled/classi\ufb01ed) point cloud data for research\nand development. Although the quantity of records in those data sets is large, and the\nvariety in the recorded surroundings and objects is suf\ufb01cient, each of the benchmark\ndata sets only uses one speci\ufb01c LiDAR sensor from one manufacturer to record the", " data.\nThe LiDAR sensors used in all of the mentioned data sets is of the same type\u2014A 360 \u00b0\nrotary LiDAR. In general, it is a good idea to make use of the same type of LiDAR sensors\nfor the different data sets, as the results of the benchmarks can be compared more easily,\nas they are not prone to calibration errors or different calibrations of the sensors. This\napproach is bene\ufb01cial for the testing and evaluation of 3D point cloud-based algorithms.\nTo investigate the practical use of different LiDAR sensors, this approach needs to be\nreversed: Different LiDAR sensors need to be evaluated in the exact same use case with the\nsame algorithms. The algorithms we settled on to evaluate our use cases are RANSAC [ 22]\nfor estimation of the position of geometric shapes in point clouds and a ray-based ground\u2013\nobject segmentation derived from the ideas from Petrovskaya and Thrun [23].\n3. Benchmark Concept\nThis section will give an overview of the benchmark concepts. In Section 3.1, we\npresent the", " requirements which are derived from real-world scenarios and show our test\ncandidates. In Sections 3.2\u20133.5, we introduce the different LiDAR targets (spheres and\nplanes) that were used during the tests and had to be detected by the sensors. Finally,\ndetails about the deployed detection algorithms will be discussed.\n3.1. Test Candidates\nHMETC selected several state-of-the-art LiDAR sensors which were available on the\nmarket or at a close-to-market series production stage. The selected sensors are either\nmechanical (moving/rotating mirror or lens parts) or solid-state types, based on micro-\nmirrors with MEMS (Micro-Electro-Mechanical Systems) technology. Flash LiDARs and\noptical phased array LiDARs could not be tested at the time of writing of the paper because\nsome of the sensors either exceeded the price limit requirement or were ruled out due to Sensors 2022 ,22, 7146 4 of 20\nother HMETC requirements, such as avoiding suppliers affected by chip shortages and\nsensors still in developm", "ent or in the validation phase.\nIn our study, we mainly focused on non-rotary LiDARs for the reason that those\nsystems do not have moving parts, which are prone to wear and tear or are more sensitive\nto challenging environmental conditions. It is planned to integrate the LiDARs into the\nvehicle front grill to have a certain height above the ground, compared to a mounting in\nthe bumper below the grill. This allows to scan a wider \ufb01eld of view in front of the vehicle\nto support object perception and localization use cases during vehicle movement (speeds\nfrom 30 km/h to80 km/h ). This is a compromise in the design philosophy to balance the\nperception needs and the impact that the mounting of the sensor in the vehicle has. It is\nalso related to the fact that HMETC test vehicle is a step-by-step implementation to achieve\nthe level of fully autonomous driving. These considerations make the mostly roof-mounted\n360\u00b0 mechanically spinning LiDARs known, for instance, from Waymo\u2019s [ 21] robotaxis", ",\nunsuitable, oversized and overpriced. Based on the described usage conditions and the\nhardware requirements given below, the sensors shown in Table 1 are tested.\nTable 1. The tested sensors with their respective scanning patterns. The pictures of the scan patterns\nwere made by facing the LiDAR toward a white wall. The screenshots were taken in the point cloud\nviewer from the LiDAR perspective.\nLivox Robosense Blickfeld Blickfeld Velodyne Innoviz\nHorizon M1 Cube Cube Range Velarray H800 Pro\nPicture\nScan pattern\nFramerate 10 Hz 10 Hz 6.3 Hz 5.7 Hz 25 Hz 16 Hz\nPoints per Frame 24.000 78.750 8.829 7.599 16.181 15.500\nFOV 81.7\u00b0 H, 25.1\u00b0 V 120\u00b0 H, 25\u00b0 V 72\u00b0 H, 30\u00b0 V 18\u00b0 H, 12\u00b0 V 120\u00b0 H, 16\u00b0 V 72\u00b0 H, 18.5\u00b0 V\nPrinciple Rotating Prisms MEMS MEMS MEMS Solid State MEMS\nWe \ufb01rst start with the different requirements for the LiDAR sensors:\n\u2022 Field of View. We aim at a horizontal \ufb01eld of view of more than 100\u00b0 and a vertical\n\ufb01eld of view of more than 10\u00b0to reduce the number of sensors that has to m", "ounted on\nthe vehicle.\n\u2022 Minimal detection distance and range. The distance to detect the LiDAR targets should\nbe less than 3 m while the range of the LiDAR should be more than 120 m.\n\u2022 Resolution and number of scan lines. The sensors should of course have a high resolution\nbelow 0.4\u00b0 and at least \ufb01ve scan lines to be able to detect the LiDAR targets and\nreal-world objects.\n\u2022 Update rate or frame rate. In order to avoid longer delays in the object detection,\nthe sensor systems should have an update frequency or frame rate of more than 5 Hz .\n\u2022 ROS/ROS2 support. For an easy integration into our control software stack [ 6], a Linux-\nbased system implementation and an AD framework based on ROS2 is preferred.\n\u2022 Robustness of sensor systems. The test candidates should work well also in tougher\nweather conditions, and the sensor performance should not notably degrade under\nthose conditions. Sensors 2022 ,22, 7146 5 of 20\nThe LiDARs were evaluated using multiple distinctive test cases, re\ufb02ect", "ing the later\nreal-world usage scenarios (outdoors, urban environment, dynamic movement, and on-\nroad). This minimizes the selection of a false-positive-rated sensor which performs well\nin a laboratory environment bench test, but shows issues with, for instance, road surface\nre\ufb02ections, ghost objects such as plants or leaves, or blinding sunlight when being used in\nreal-world environments. Mounting each sensor to a test vehicle can also reveal possible\nnegative impacts, such as (electromagnetic) interference with other test vehicle components,\nincluding measurement units or ECU-related signal transmission delays, such that the per-\nformance of a sensor might degrade to a level not acceptable in an automotive environment.\nThe sensors in Table 1 were selected based on the following criteria:\n\u2022 Packaging size, interface and cabling;\n\u2022 Sensor IP rating: Robustness against water, rain, snow, mud, stones\n\u2022 Setup time, run-time without failure;\n\u2022 Power consumption: Low power consumption (less", " than 15 W per unit);\n\u2022 Sensor availability;\n\u2022 Manufacturer support;\n\u2022 Con\ufb01guration options;\n\u2022 Scan pattern to ensure similar test conditions.\nThe sensors were provided by HMETC for running the benchmark activities.\nMost sensors have the possibility to change FPS (frames per second) and/or points\nper frame. Some even allow changes in the scanning pattern. For our tests, we selected\na number of LiDARs with very speci\ufb01c patterns that moreover meet the requirements\npresented in the previous section. In the next sections, we introduce how we detect the\ndifferent LiDAR targets.\n3.2. Scenario 1: Static Spheres in Increasing Distances\nIn this section, the \ufb01rst test scenario is presented, where a triangular-shaped sphere\ntarget has to be detected by the LiDAR system in different distances, ranging from 7 mto\n25 m. The vehicle is not in motion for this scenario.\nThe deployed LiDAR target is shown in Figure 1. A structure of three spheres aligned\nin an equilateral triangle is used. Each sphere h", "as a diameter of 40 cm. The space between\neach sphere center point is approximately 110 cm . To detect the LiDAR target, RANSAC [ 22]\nis being applied to \ufb01nd the spheres inside the point cloud. Each time the algorithm detects\nexactly all three spheres, the centroid of the triangle is calculated, containing the measured\ndistances in the point cloud. The process repeats ntimes; the result is a dataset with n\nvalues for the centroid (see Figure 1a) because a measurement was only added to the\ndataset if all the spheres were detected.\nIn this scenario, we measure the performance of the LiDAR sensors for increasing\ndistances, while the distance does not change during the measurement, i.e., LiDAR sensor\nand measured objects are standing still. Four different distances between the LiDAR sensor\nand the sphere construction were used: 7 m,12 m ,15 m , and 25 m . The minimum distance\nwas chosen because at 7 mall LiDAR had all spheres in their FOV . The maximal distance\nwas chosen after noticing th", "at at this range the \ufb01rst sensors were unable to detect the\nspheres; knowing the performance at 25 m was a good indication for the following dynamic\nscenarios. The decision to use 12 m and 15 m in between instead of equidistant distances\nhas no particular reasons. For each distance and for each LiDAR sensor, 1000 data points\nwere taken (cf. Section 4.1 for the results). Sensors 2022 ,22, 7146 6 of 20\n40 cm110 cm110 cm\n40 cm40 cm\nCen tr oid\n(a)\n (b)\nxLiDAR\ny\n7 m - 25 m  \n(c)\nxzLiDAR\n7 m - 25 m (d)\nFigure 1. Scenario 1 setup: ( a) Construction sketch. ( b) Picture of the construction. ( c,d) Topview and\nsideview of the measurement setup.\n3.3. Scenario 2: Static Square Meter Reference Plane\nIn the second scenario, errors that might have been introduced by deploying RANSAC\nfor the sphere detection in Scenario 1 should be avoided and should justify the validity of\nthe other scenarios. If a deployed detection algorithm favours or disfavours a particular\nLiDAR sensor, can be identi\ufb01ed with th", "is scenario. We constructed a free-\ufb02oating square\nmeter plane as shown in Figure 2a. The wooden square meter plane was set up 7 min front\nof the LiDAR sensors (both standing). As the laser beams will just hit the front of the square\nmeter plane, the plane will create free \ufb02oating points inside the point cloud. Around these\npoints, a cut-out box is placed (virtually inside the point cloud), and all points that are not\ninside this box are extracted from the point cloud (Figure 2b). The remaining points inside\nthe point cloud are just the points originated by the re\ufb02ection of the square meter plane\nitself. Inside the remaining point cloud, the minima and maxima in every direction can be\ndetermined, and we can be establish the following equations:\nzAxis max\u0000zAxis min\u00191 m\nyAxismax\u0000yAxismin\u00191 m\nxAxis max\u0000xAxis min\u00190 m\nSo, we expect to measure the width and the height of the plane ( 1 m inzand y\ndimension). Of course, the difference in the distance to the plane should be 0(xdimension).\nRefer ", "to Figures 2c and d for the naming of the axes. The measured sizes will scatter around\nthe true size and with 1000 measurements we approximate a normal distribution of the\nvalues just like with Scenario 1. The results are shown in Section 4.2. Sensors 2022 ,22, 7146 7 of 20\n(a)\n0.8 m 0.6 m\n2.5 m1.0 m1.0 m (b)\nxy \n1 m\n7 mLiDAR\n(c)\nxzLiDAR\n7 m1m (d)\nFigure 2. Scenario 2: ( a) Square meter plane target. ( b) Point cloud of a detected target. Red points\nare points left over after cutting out a box around the plane. The box is spaced 1 m below and 1 m\nabove the plane, 60 cm in front and 80 cm behind. It is 2.5 m wide around the center of the plane.\n(c,d) Topview and sideview of the measurement setup.\n3.4. Scenario 3: Dynamic Spheres\nThis scenario is used to measure the performance of the LiDAR sensors for decreasing\ndistances, while the LiDAR moves toward the statically placed sphere construction during\nthe measurement (dynamic measurement), as shown in Figure 3. The LiDAR sensor was\nmounte", "d on top of a test vehicle The vehicle started 300 m away from the measurement\ntarget, and drove automatedly toward the sphere construction with a speed of 10 km/h .\nThe speed of 10 km/h was chosen for both dynamic scenarios (Scenario 3 and 4) to cater\nfor the update rates of the LiDAR sensor. Other tests with 80 km/h were made, but were\nlater excluded from both dynamic scenarios because of the distortion problem which will\nbe discussed in Section 4.3. For each measurement, the vehicle drove at a constant speed.\nThe position of the sphere construction and the position of the test vehicle was measured by\nGPS. Our detection algorithm measured the distance to the triangular sphere construction\n(distance lidar) at driving distance x. As a reference, the distance between the vehicle\u2019s GPS\nposition and the triangular sphere construction\u2019s GPS position was calculated as distance gps.\nNow, the difference between the distance measured by LiDAR and by GPS at any given\ndistance can be calculated ", "as:\ndistance D(x) =distance lidar(x)\u0000distance gps(x) (1)\nNote that distance Dwill not become zero due to the offset between the vehicle\u2019s GPS\nposition the position of the LiDAR sensor. Sensors 2022 ,22, 7146 8 of 20\n(a)\n (b)\nTriangular  spher e construc\u0000on\nVehicle  LiDAR Sensor  Driving dir ec\u0000onLiDARGPS  \nVehicleGPS  \nPoint\nSpheres\ndistance_GPS\ndistance_LiDAR\n(c)\nFigure 3. Scenario 3: ( a) Spheres on the test track; ( b) LiDAR mounted to the test vehicle; ( c) Topview\nof the dynamic test scenario.\n3.5. Scenario 4: Dynamic Vehicle Detection\nThis scenario is designed to be close to a real-world use case. This scenario is designed\nanalogously to Scenario 3, but instead of the sphere construction, another still-standing\nvehicle was placed on the shoulder lane and was to be detected by the LiDAR sensor (see\nFigure 4). Again as in Scenario 3, we used Equation (1) to measure the offset between the\ntrue position and the detected position of the target.\nThe algorithms used in the previous scen", "arios could not be used for object detection,\nbecause it is only capable of detecting spheres. Therefore, another algorithm had to be\ndeveloped. Using a ray-based ground object segmentation [ 23], the algorithm converts\nthe point cloud into a 2D grid of cells. For each cell of the grid, the algorithm calculates\na height pro\ufb01le (see Figure 5). The height of a cell is equal to the highest measured point\nof the point cloud inside the grid. Once each cell has a height pro\ufb01le, the cell extension\nstarts. Starting from the origin and then going up-stream (in the driving direction, where\nthe bottom of the grid is nearest to the vehicle), the algorithm calculates an angle between\ntwo grid cells (see side view of Figure 5). The calculated angle is compared to an angle\nthreshold. If the angle between two cells is bigger than the threshold, the cell is \ufb02agged as\nnon-ground. This means it contains a whole object or only a part of an object bigger than\nthe cell. In the end, the resulting point cloud", " with all the non-ground-\ufb02agged cells is split\ninto single objects by extracting the resulted cluster. The results of Scenario 4 are shown in\nSection 4.4. Additionally, for the difference between the measured distance by LiDAR and\nthe measured distance by GPS, also the width and the height of the second vehicle were\nmeasured with the LiDAR. Sensors 2022 ,22, 7146 9 of 20\n(a)\nVehicle  LiDAR Sensor  Driving dir ec\u0000onLiDARGPS  \nVehicle\nGPS  \nPoint\nSecond\nvehicleSecond v ehicledistance_GPS distance_LiDAR\n(b)\nFigure 4. Scenario 4 ( a) A second still-standing vehicle is the detection target. ( b) Top view of the\nScenario 4.\nWave expansion\nabGrid\nTopview\ncAngle threshold\nab c \nAngle a \u2192 b <  threshold Angle b \u2192 c > thresholdGrid\nSideview\nFigure 5. Ray-based ground\u2013object segmentation detection functionality, sketch. The angle between\ncell a and b does not exceed the threshold, cell b is marked as ground cell. The angle between cell b\nand c exceeds the threshold, cell c is marked as non-ground", ". Sensors 2022 ,22, 7146 10 of 20\n4. Results\nThis section will show the results and will give insights on the comparison, the de-\ncisions that were made, and some compelling problems that occurred. In all scenarios,\nmultiple datasets were captured. A dataset for one measurement will scatter around an\naverage, resulting in a normal distribution. The static scenarios are evaluated by com-\nparing standard deviations. For dynamic scenarios, the measured points correlate with\nthe distance.\n4.1. Scenario 1: Static Spheres Increasing Distances\nFigure 6 shows the standard deviations according to each axis and distance. The Blick-\nfeld Cube as well as the Innoviz Pro failed to perform in this scenario at a distance of\n25 m ; hence, the decision was made to exclude both of them from further investigations\nin dynamic scenarios. Even though the Blickfeld Cube Range yielded good results, it was\nexcluded for dynamic scenarios as well, due to its too narrow \ufb01eld of view.\nDespite being accurate at the", " 7 mdistance, the Blickfeld Cube sensor shows less accuracy\nat higher distances and fails to measure at 25 m . This is due to its small amount of available\nscan points per frame. It is noticeably lower than all the others (see data sheets in Table 1).\nThe Blickfeld Cube Range on the other hand shows the highest accuracy in this test due to\nits zoom lens. At a distance of 15 m it is about as accurate as the Blickfeld Cube at 7 m.\nThe zoom lens helps to keep the beams close together resulting in a dense scanning pattern\neven in higher distances. However, the dense scanning pattern does, on the other hand,\nresult in a too narrow FOV and therefore the sensor was excluded from dynamic tests.\nThe Velodyne Velarray performed with an average result. There is a noticeable decrease\nin the accuracy between 7 mto12 m . Compared to its competitors, the accuracy of the\nRobosense M1 was decreasing less with higher distances. The reason for this could be its\nreally high amount of points and homogeneou", "s scan pattern with the same amount of\nbeams in vertical and horizontal alignment. Finally, the Livox Horizon provided a high\naccuracy at 7 mto15 m distance, while at a distance of 25 m , the accuracy decreases. Fewer\npoints hitting the sphere at this distance lead to a smaller accuracy. It shows marginal noise\nfor each point; this also explains the accurate measurements at closer distances.\n4.2. Scenario 2: Static Square Meter Reference Plane\nFigure 7 shows the measured standard deviations for each LiDAR in x,yand z\ndirection for this scenario. The x-axis refers to the distance to the square meter plane,\nthey-axis to the width and the z-axis to the height of the plane. With the Blickfeld Cube and\nthe Velarray sensors, we observed a low accuracy for the distance measurement ( x-axis).\nWe explain this observation in detail in the next paragraph and will refer to it as the edge\nfringing problem in the rest of the paper. The problem did not occur with the Velarray or the\nBlickfeld Cube. S", "ensors 2022 ,22, 7146 11 of 20\nDistance - 7 m\nBlickfeld Cube\nBlickfeld Cube RangeInnovizPro\nVelodyne VelarrayRobosense M1LivoxHorizon00.10.20.30.4Standard Deviation [cm]x-Axis y-Axis z-Axis\nDistance - 12 m\nBlickfeld Cube\nBlickfeld Cube RangeInnovizPro\nVelodyne VelarrayRobosense M1LivoxHorizon00.10.20.30.4Standard Deviation [cm]x-Axis y-Axis z-Axis\nDistance - 15 m\nBlickfeld Cube\nBlickfeld Cube RangeInnovizPro\nVelodyne VelarrayRobosense M1LivoxHorizon00.10.20.30.4Standard Deviation [cm]x-Axis y-Axis z-Axis\nDistance - 25 m\nBlickfeld Cube\nBlickfeld Cube RangeInnovizPro\nVelodyne VelarrayRobosense M1LivoxHorizon00.10.20.30.4Standard Deviation [cm]x-Axis y-Axis z-Axis\nFigure 6. Results: Standard deviations for each individual axis in scenario \u2019Static Spheres Increasing\nDistances\u2019 at 7 m, 12 m, 15 m and 25 m; normal distribution with 1000 samples. Sensors 2022 ,22, 7146 12 of 20\nBlickfeld Cube Blickfeld Cube Range InnovizPro Velodyne Velarray Robosense M1 LivoxHorizon00.010.020.030.040.050.060", ".07Standard Deviation [cm]x-Axis y-Axis z-Axis\nFigure 7. Results: Standard deviations for each individual axis in scenario \u2019Static Square Meter\nReference Plane\u2019 normal distribution, 1000 samples.\nIn Figure 8, we explain the edge fringing problem. As can be noticed on the edges of\nthe plate, points are falsely detected behind the plate. This effect was strongest at the top\nand the bottom (see Figure 8). This phenomenon was observed with almost all sensors.\nIn order to \ufb01gure out what was causing that problem, a sensor was turned 90 degrees on its\nx-axis while scanning the plate. Prior to the rotation, the problem was seen at the top and\nthe bottom; after the rotation, the problem was seen on the sides of the plate. This means it\nwas caused by the sensor\u2019s scan pattern, not the wooden plate itself. This test was repeated\nwith another sensor showing the same behavior.\nThe Blickfeld Cube showed a higher accuracy in x-axis measurements than its competi-\ntors, while showing bad results for th", "e yand z-axis accuracy. Despite having the same\nhardware, the Blickfeld Cube Range did not yield the same results as the Blickfeld Cube.\nThe accuracy of y- and z-axis are noticeably more accurate than the x-axis measurements,\nwhich can be traced back to the edge fringing problem. The Innoviz Pro showed less accurate\nx-axis measurements, again because of the edge fringing problem, while the Velodyne Velar-\nrayachieved the best results, providing the most accurate measurements for all three axis.\nThe sensor suffering the most from the edge fringing problem was the Robosense M1 , while\ntheLivox Horizon performed with an average accuracy despite the edge fringing problem.\nLiDAR scan direction Fringing\nWood plate\nFigure 8. The red point cloud contains the re\ufb02ections of the reference plane. The fringing at the top is\nnoticeable (marked in the green circle), which is also known as edge fringing. Sensors 2022 ,22, 7146 13 of 20\n4.3. Scenario 3: Dynamic Spheres\nDuring the measurements, a proble", "m was discovered that would cause the results of\nthe dynamic scenarios to be unreliably. When LiDAR sensors are moved, the point cloud\nbecomes distorted. The distortion could, theoretically, be detected and corrected, but due to\nthe data structure of the submitted source date of some LiDAR sensors, it was not possible\nto correct this distortion.\nMoving a sensor at high velocities causes distortion. As the sensor traverses more\ndistance in the time period it takes to measure a complete point-cloud, more distortion is\nintroduced to the measured scene. This is caused by the sequential transmission of the\nscanned points. The time difference in between measured points allows for a relative error\nin the opposite direction of motion. This effect, as seen in Figure 9, is clearly visible when\nobserving clusters of points measured on static objects lateral to the direction of movement.\nEarly points in the point-cloud seem to be further away than points measured later in the\nmeasurement-period, r", "esulting in the pattern representing an object being stretched along\nthe axis of movement. With such distortion, the algorithm was not able to detect spheres\nin an adequate extent, even at low speeds. It was not possible to extract the distortion for\nsome LiDARs (as discussed in Sections 5 and 6); therefore, the decision was made to leave\nthe results out, as they may be unrepresentative.\n(a)\n (b)\n (c)\nFigure 9. This \ufb01gure presents the visualization of the distortion effect due to movement of the\nsensor at high velocities. ( a) Picture of target. ( b) View at 10 km/h , Visualization of the measured\npoint-cloud at 10 km/h rotated so the structure can be viewed from the left side. ( c)View at 80 km/h ,\nThe visualization of the measured point-cloud at 80 km/h rotated so the structure can be viewed\nfrom the left side. In ( c), the measured structure is virtually stretched over a span of 2.22 m.\n4.4. Scenario 4: Dynamic Vehicle Detection\nThis test was conducted with the selected top three se", "nsors out of the previous scenar-\nios. These three yielded best results so far and are best suited for Scenario 4:\n\u2022 Robosense M1;\n\u2022 Velodyne Velarray H800;\n\u2022 Livox Horizon.\n4.4.1. Position Difference Test 10 km/h\nFigure 10 shows the difference between the distance to the second vehicle measured\nvia GPS against the distance to the second vehicle measured by each LiDAR in relation\nto the distance. Because the LiDARs were mounted in the front of the car roof and the\nGPS measurements are taken at the back axle of the measurement vehicle (see Figure 4),\nan offset of about 2.5 m occurs, which leads to the curve seen in Figure 10. Sensors 2022 ,22, 7146 14 of 20\n050100150200250300\nDistancetovehicle[m]-1012345DeltabetweenGPSandmeasuredposition[m]MeasurementPoint\nOffsetRecreation\n(a)\n050100150200250300\nDistancetovehicle[m]-1012345DeltabetweenGPSandmeasuredposition[m]MeasurementPoint\nOffsetRecreation\n(b)\n050100150200250300\nDistancetovehicle[m]-1012345DeltabetweenGPSandmeasuredposition[m]Measure", "mentPoint\nOffsetRecreation\n(c)\nFigure 10. Results of Scenario 4. The difference between GPS and LiDAR measurement in relation to\nthe distance is shown ( a) Velodyne Velarray (1175 vehicle detections); ( b) Robosense M1 (522 vehicle\ndetections); ( c) Livox Horizon (468 vehicle detections). Sensors 2022 ,22, 7146 15 of 20\nFigure 10a shows the differences in the position measurements for the Velodyne Velarray .\nBecause the sensor has a sample rate of 20 FPS, it generated most of the detection of the\nvehicle. The sensor is capped by its software driver to 200 m , which is the reason why the\nsensors measurements never exceeded this distance. The sensor managed a few detections\nup to 150 m , where detections occur more often. The results Robosense M1 are shown in\nFigure 10b. This sensor is is also capped at 200 m by the sensors driver. Besides some\noutliers, the sensor shows overall precise and dense measurements up to 150 m . The Livox\nHorizon (Figure 10c) was the only sensor that was able ", "to detect the car at a distance of\nover 250 m . Besides the overall good result in range, the sensor measured the position less\naccurately and less densely than its competitors.\n4.4.2. Height Measurement Test 10 km/h\nFigure 11 shows the height of the second standing vehicle measured by the LiDAR\nin relation to the distance. The actual height of the measured car was 1.7 m . Height mea-\nsurements are less important for automotive use cases; hence the vertical resolution is\noften compromised. A low vertical resolution leads to a phenomenon called quantization.\nAt far distances of around 150 m , just a few points re\ufb02ect from the measured second vehicle.\nThe result of the height measurement relies on the highest point that was re\ufb02ected and\nis therefore only an approximation that will get better the closer the LiDAR gets to the\nmeasured object. The same applies to width measurements, even though the horizontal\nresolution is often higher.\n0 50 100 150 200 250 300\nDistance to vehicle [m]00.511", ".522.5Measured height of vehicle [m]Actual HeightMeasurement Point\n(a)\n0 50 100 150 200 250 300\nDistance to vehicle [m]00.511.522.5Measured height of vehicle [m]Actual HeightMeasurement Point\n(b)\nFigure 11. Cont . Sensors 2022 ,22, 7146 16 of 20\n0 50 100 150 200 250 300\nDistance to vehicle [m]00.511.522.5Measured height of vehicle [m]Actual HeightMeasurement Point\n(c)\nFigure 11. Results for Scenario 4. Measured height in relation to the distance to the second vehicle.\n(a) Velodyne Velarray (1175 vehicle detections); ( b) Robosense M1 (522 vehicle detections); ( c) Livox\nHorizon (468 vehicle detections).\nThe Velodyne Velarray has a very low vertical resolution. However, for an automotive\nsensor, this is nonetheless acceptable. At a distance of approximately 120 m , the points\nare80 cm apart vertically, con\ufb01rming the pattern seen in Figure 11a, as the probability\nat this distance that a point of a higher scan line does hit the standing vehicle increases.\nThe Robosense M1 has a wider vert", "ical FOV alongside a higher vertical resolution than the\n\u2019Velodyne Velarray\u2019, being less prone to the quantization problem in the vertical direction.\nOther than its competitors does the Livox Horizon not use a common grid-like scan pattern.\nThe deployed scan pattern has bene\ufb01ts, as the probability is increased that some points\nhit the second vehicle at a larger height. Figure 11c does not show the same pattern as\npreviously seen with the other LiDARs, which have a unique scan pattern .\n4.4.3. Width Measurement Test 10 km/h\nFigure 12 shows the width of the second standing vehicle measured by the LiDAR\nin relation to the distance. The actual width of the measured car is 1.9 m . In traf\ufb01c,\nwidth and length of objects are often more important, because cars can only move in a 2D\nspace. The quantization phenomenon explained previously will again have an impact on\nthe measurements. It has to be noted that the LiDAR systems detect the second vehicle\nstanding on the side, not directly from behi", "nd. Therefore the sensors mostly measure the\ntrue width, but some beams will hit the side of the vehicle as well, which falsely causes a\nhigher width. This explains why all tested LiDAR systems measurements of the vehicle are\na bit wider than they actual are.\nThe width measurement of the Velodyne Velarray is more accurate than its height\nmeasurement. Its measurements are consistent, taking the quantization pattern into account.\nCompared to Velarray, the Robosense M1 is closer to the real width of the second vehicle\nat a distance of 80 m . This makes Robosense and Velarray close competitors in this test.\nFinally, the detection of Livox Horizon does not follow the quantization pattern and has\na lot of variations. This could be explained by its scan pattern: sometimes, the beams\nscan just a part of the width of the car, but going up or down before scanning the whole\ncar, causing the less accurate measurements and out-of-place-looking points below 1 m in\nwidth. Additionally, it is the only", " sensor that is able to return a width near ground truth at\nover 200 m. Sensors 2022 ,22, 7146 17 of 20\n0 50 100 150 200 250 300\nDistance to vehicle [m]00.511.522.53Measured width of vehicle [m]Actual WidthMeasurement Point\n(a)\n0 50 100 150 200 250 300\nDistance to vehicle [m]00.511.522.53Measured width of vehicle [m]Actual WidthMeasurement Point\n(b)\n0 50 100 150 200 250 300\nDistance to vehicle [m]00.511.522.53Measured width of vehicle [m]Actual WidthMeasurement Point\n(c)\nFigure 12. Results: Scenario \u2019Dynamic Vehicle Detection\u2019. Measured width in relation to the distance\nto the second vehicle. ( a) Velodyne Velarray (1175 vehicle detections); ( b) Robosense M1 (522 vehicle\ndetections); ( c) Livox Horizon (468 vehicle detections).\n5. Discussion\nIn this section, we will discuss the observations we made during the benchmark.\nFurther, we will discuss a number of problems that occurred with the tested sensors.\nBased on the results of the first scenario, static spheres increasing distances ( ", "Section 4.1 ),\nwe decided to exclude three sensors from dynamic scenarios. Both the Blickfeld Cube Sensors 2022 ,22, 7146 18 of 20\nand the Innoviz Pro were not able to perform the benchmark at a distance of 25 m and\nthe Blickfeld cube range showed that the narrow FOV would be too narrow for the dy-\nnamic scenarios.\nIn Scenario 2 (Section 4.2), we identi\ufb01ed a problem which led to poor performance in\nmeasuring the xdirection of most sensors. This fringing problem at the edges of the plane,\nreferred to in Section 4.2, was documented and forwarded to the manufacturers. At this\npoint, the reason for this phenomenon is unclear and just speculative.\nScenario 3 (Section 4.3) could not be executed because of the distortion problem. This\nproblem is inherent for all LiDARs and will affect sensors with low refresh rates more\nthan sensors with high refresh rates. The distortion can be removed by calculating the\nmovement of the sensor against the delta between points, but the data structures of some", "\npoint clouds have dynamic sizes, rendering this approach impossible. Here, it needs to be\nmentioned that Livox provides an open source software solution to remove the distortion\nby deploying the integrated IMU of the sensor.\nIn the last scenario (Section 4.4), the obvious observation was made that for large\ndistances, the \ufb01xed gaps between the scan lines of the scan patterns makes precise object\ndetection dif\ufb01cult. The Livox Horizon LiDAR sticks out because of the unconventional\nscan pattern. With this, it outperforms its competitors in terms of range, while, on the other\nhand, losing precision in comparison to Robosense M1 and Velodyne Velarray H800. Scan\npatterns and point density also make a difference when measuring the dimension of objects.\nThe observation we made here was that the unconventional scan pattern design of Livox\ncan help to obtain a better approximation for the real dimensions of the scanned object.\nSensors designed for automotive applications tend to trade off verti", "cal against hor-\nizontal FOV as can be seen with the Velodyne Velarray H800. Summarizing, we found\nthat the information of the properties given in the data sheet is of course important and\nvaluable, but it does not necessarily yield reliable information on how this sensor works\nin practice.\nAnother observation was that unconventional scan patterns have their bene\ufb01ts. LiDAR\nsensors have a low resolution in comparison to cameras; with conventional grid-like scan\npatterns, a low resolution results in big gaps between the beams. Unconventional scan\npatterns can counteract these gaps.\nDuring the evaluation of the LiDAR sensors, we encountered multiple possible prob-\nlems with the data, which prevent or may in\ufb02uence the performance of the sensors. Some of\nthese occurred only on devices from speci\ufb01c manufacturers, while others can be corrected\nin post-processing steps.\nAll of the analyzed sensors had a certain level of noise . The noise observed on indi-\nvidual point measurements deviated in ", "many cases from the expected location of where a\nsingle beam of the sensor would re\ufb02ect off. In other cases, the location of the measured\npoints was contradictory. Points were measured in mid-air with no obvious obstacle in\nsight. Speci\ufb01cally in medium and far-away regions of the point cloud, the points contained\nfaulty or non-usable information. This can be a hazard if the algorithms used are supposed\nto detect patterns and objects from distant measurements. The faulty information could\noccasionally form clusters of points, which can be similar to a search pattern. For exam-\nple, in a collision avoidance system, false positives due to noise can endanger the life of\npassengers if safety precautions, such as the fastening of a seat belt, are not taken. A false\npositive in the near \ufb01eld could result in an hard braking manoeuver. Not only could this\ncause damage to the passengers, but it could also potentially lead to loss of control over the\nvehicle by the driver.\nThe edge fringing effec", "t of different LiDAR sensors shown in Figure 8 and explained\nin the respective section can be compensated through the con\ufb01guration of the individual\n\ufb01rmware. Different measurement types are available to decide by which strategy the\nre\ufb02ected beams will be evaluated. The measurement type \u201cstrongest return\u201d results in\nthe re\ufb02ected beams with the most intensity to be chosen for the calculation of a 3D point. Sensors 2022 ,22, 7146 19 of 20\nAnother measurement type \u201c\ufb01rst return\u201d leads to the \ufb01rst re\ufb02ected beam to be chosen for\nthe same calculation.\nWe want to point out that some problems should be addressed by the manufacturers;\nwhen asked about the fringing and distortion problem, the manufacturer\u2019s technological\nsupport was often unaware that such problems exist or had to speculate.\n6. Conclusions\nDeploying LiDAR technology has major bene\ufb01ts in many robotics application \ufb01elds,\nas well in the \ufb01eld of automated driving. Series production of automated vehicles would\ndemand sensors that are p", "recise and in an acceptable price range. A set of different LiDAR\nsensors were selected to be benchmarked against each other in four different scenarios.\nTo the best of our knowledge, benchmark scenarios based on real-life use cases have not\nbeen proposed in the literature before. We divided the scenarios into static and dynamic\ntests. In static scenarios, both the measured object and the sensor did not move; in the\ndynamic scenarios, the sensor was placed on a vehicle that drove toward the measured\nobject. In contrast to other benchmarks such as [ 11], the selected LiDARs were mainly\nbased on solid-state LiDAR technology.\nThe \ufb01ndings in this paper have shown that there are considerable differences in LiDAR\ntechnologies: for individual use cases, the whole package has to be considered, including\nthe availability and the kind of driver software. As additional software may also be required\nand useful, open source software should be the \ufb01rst choice.\nScenario 2 ( static spheres in increasi", "ng distances , Section 4.1) and especially the static\nsquare meter reference plane scenario in Section 4.2 show that the tested LiDAR sensors can\nhave major deviations in point precision.\nScenario 4 ( dynamic vehicle detection , Section 4.4) shows that the scan pattern of a LiDAR\ncan make a difference, an observation of which researchers and developers seem not to\nbe very aware. When designing a LiDAR-based use case, the scan pattern should not be\nignored. The results of this publication help to select the best-suited LiDAR for a particular\napplication. Further, the minor and major differences between the tested LiDAR sensors\nand their particular technologies become apparent.\nAuthor Contributions: Conceptualization, J.S.-T., M.R. and A.F.; software, M.F.; validation, J.S.-T.,\nM.F. and D.K.; resources, D.M. and D.K.; data curation, J.S.-T.; writing\u2014original draft preparation,\nJ.S.-T. and M.F.; writing\u2014review and editing, A.F., M.R., J.S.-T. and G.N.; visualization, J.S.-T.;\nsupervision,", " T.W.; project administration, M.R. and T.W. All authors have read and agreed to the\npublished version of the manuscript.\nFunding: This manuscript received no external funding.\nInstitutional Review Board Statement: Not applicable.\nInformed Consent Statement: Not applicable.\nData Availability Statement: Data available on request due to privacy restrictions. The data pre-\nsented in this study are available on request from the corresponding authors. The data are not\npublicly available due to company policies.\nCon\ufb02icts of Interest: The authors declare no con\ufb02ict of interest.\nReferences\n1. Driving, A. Levels of Driving Automation are De\ufb01ned in New SAE International Standard J3016: 2014; SAE International: Warrendale,\nPA, USA, 2014.\n2. First Internationally Valid System Approval for Conditionally Automated Driving. Daimler. Available online: https:\n//www.daimler.com/innovation/product-innovation/autonomous-driving/system-approval-for-conditionally-automated-\ndriving.html (accessed on 13 Dece", "mber 2021).\n3. Roriz, R.; Cabral, J.; Gomes, T. Automotive LiDAR Technology: A Survey. IEEE Trans. Intell. Transp. Syst. 2022 ,23, 6282\u20136297.\ndoi: 10.1109/TITS.2021.3086804. [CrossRef]\n4. Pendleton, S.D.; Andersen, H.; Du, X.; Shen, X.; Meghjani, M.; Eng, Y.H.; Rus, D.; Ang, M.H., Jr. Perception, planning, control,\nand coordination for autonomous vehicles. Machines 2017 ,5, 6. [CrossRef] Sensors 2022 ,22, 7146 20 of 20\n5. Buehler, M.; Iagnemma, K.; Singh, S. The DARP A Urban Challenge: Autonomous Vehicles in City Traf\ufb01c ; Springer: Berlin/Heidelberg,\nGermany, 2009; Volume 56.\n6. Reke, M.; Peter, D.; Schulte-Tigges, J.; Schiffer, S.; Ferrein, A.; Walter, T.; Matheis, D. A self-driving car architecture in ROS2. In\nProceedings of the 2020 International SAUPEC/RobMech/PRASA Conference, Cape Town, South Africa, 29\u201331 January 2020;\npp. 1\u20136.\n7. Hi Drive, EU Project 101006664. 2021. Available online: https://www.hi-drive.eu (accessed on 22 July 2022).\n8. Arnold, E.; Al-Jarrah, O.Y.; Dianati, M", ".; Fallah, S.; Oxtoby, D.; Mouzakitis, A. A Survey on 3D Object Detection Methods for\nAutonomous Driving Applications. IEEE Trans. Intell. Transp. Syst. 2019 ,20, 3782\u20133795. doi: 10.1109/TITS.2019.2892405.\n[CrossRef]\n9. Elhousni, M.; Huang, X. A Survey on 3D LiDAR Localization for Autonomous Vehicles. In Proceedings of the 2020 IEEE Intelligent\nVehicles Symposium (IV), Las Vegas, NV , USA, 19 October\u201313 November 2020; pp. 1879\u20131884. doi: 10.1109/IV47402.2020.9304812.\n[CrossRef]\n10. Zou, Q.; Sun, Q.; Chen, L.; Nie, B.; Li, Q. A Comparative Analysis of LiDAR SLAM-Based Indoor Navigation for Autonomous\nVehicles. IEEE Trans. Intell. Transp. Syst. 2021 ,23, 6907\u20136921. doi: 10.1109/TITS.2021.3063477. [CrossRef]\n11. Lambert, J.; Carballo, A.; Cano, A.M.; Narksri, P .; Wong, D.; Takeuchi, E.; Takeda, K. Performance Analysis of 10 Models of 3D\nLiDARs for Automated Driving. IEEE Access 2020 ,8, 131699\u2013131722. doi: 10.1109/ACCESS.2020.3009680. [CrossRef]\n12. Cattini, S.; Cassanelli, D.; Cecilia, ", "L.D.; Ferrari, L.; Rovati, L. A Procedure for the Characterization and Comparison of 3-D LiDAR\nSystems. IEEE Trans. Instrum. Meas. 2021 ,70, 1\u201310. doi: 10.1109/TIM.2020.3043114. [CrossRef]\n13. Bijelic, M.; Gruber, T.; Ritter, W. A Benchmark for Lidar Sensors in Fog: Is Detection Breaking Down? In Proceedings of the\n2018 IEEE Intelligent Vehicles Symposium (IV), Changshu, China, 26\u201330 June 2018; pp. 760\u2013767. doi: 10.1109/IVS.2018.8500543.\n[CrossRef]\n14. SPIE Article on LiDAR Benchmarking Tests. Available online: https://spie.org/lidar-test?SSO=1 (accessed on 22 July 2022).\n15. Shi, S.; Guo, C.; Jiang, L.; Wang, Z.; Shi, J.; Wang, X.; Li, H. Pv-rcnn: Point-voxel feature set abstraction for 3d object detection.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle, WA, USA, 14\u201319 June 2020;\npp. 10529\u201310538.\n16. Lang, A.H.; Vora, S.; Caesar, H.; Zhou, L.; Yang, J.; Beijbom, O. Pointpillars: Fast encoders for object detection from point clouds.\nIn Pro", "ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 16\u201320 June\n2019; pp. 12697\u201312705.\n17. Shi, S.; Wang, X.; Li, H. Pointrcnn: 3d object proposal generation and detection from point cloud. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 16\u201320 June 2019; pp. 770\u2013779.\n18. Nikolovski, G.; Reke, M.; Elsen, I.; Schiffer, S. Machine learning based 3D object detection for navigation in unstructured\nenvironments. In Proceedings of the 2021 IEEE Intelligent Vehicles Symposium Workshops (IV Workshops), Nagoya, Japan,\n11\u201317 July 2021; pp. 236\u2013242. doi: 10.1109/IVWorkshops54471.2021.9669218. [CrossRef]\n19. Geiger, A.; Lenz, P .; Stiller, C.; Urtasun, R. Vision meets robotics: The kitti dataset. Int. J. Robot. Res. 2013 ,32, 1231\u20131237.\n[CrossRef]\n20. Caesar, H.; Bankiti, V .; Lang, A.H.; Vora, S.; Liong, V .E.; Xu, Q.; Krishnan, A.; Pan, Y.; Baldan, G.; Beijbom, O. nuscenes: A\nmultimodal data", "set for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, Seattle, WA, USA, 13\u201319 June 2020; pp. 11621\u201311631.\n21. Sun, P .; Kretzschmar, H.; Dotiwalla, X.; Chouard, A.; Patnaik, V .; Tsui, P .; Guo, J.; Zhou, Y.; Chai, Y.; Caine, B.; et al. Scalability in\nperception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, Seattle, WA, USA, 14\u201319 June 2020; pp. 2446\u20132454.\n22. Fischler, M.A.; Bolles, R.C. Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and\nAutomated Cartography. Comm. ACM 1981 ,24, 381\u2013395. doi: 10.1145/358669.358692. [CrossRef]\n23. Petrovskaya, A.; Thrun, S. Model based vehicle tracking for autonomous driving in urban environments. In Proceedings of the\nRobotics: Science and Systems IV , Zurich, Switzerland, 25\u201328 June 2008; p. 34."]