{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -Uqqq pip --progress-bar off\n",
    "# %pip install -qqq ollama --progress-bar off\n",
    "\n",
    "\n",
    "# %pip install -qqq owlready2 --progress-bar off\n",
    "\n",
    "# %pip install -qqq langchain-ollama --progress-bar off\n",
    "# %pip install -qqq langchain-community --progress-bar off\n",
    "# %pip install -qqq pypdf --progress-bar off\n",
    "\n",
    "# %pip install -qqq faiss-cpu --progress-bar off\n",
    "# %pip install -qqq rank_bm25 --progress-bar off\n",
    "# %pip install -qqq fuzzywuzzy --progress-bar off\n",
    "# %pip install -qqq scikit-learn --progress-bar off\n",
    "# %pip install -qqq sentence-transformers --progress-bar off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohit\\Documents\\Rohit\\Dev\\Thesis\\.conda\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\rohit\\Documents\\Rohit\\Dev\\Thesis\\.conda\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import ollama\n",
    "import json\n",
    "from enum import Enum\n",
    "import json\n",
    "import re\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.chat_models import ChatOllama\n",
    "from owlready2 import *\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from fuzzywuzzy import fuzz\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers import EnsembleRetriever, BM25Retriever, MultiQueryRetriever\n",
    "from langchain.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SentenceTransformer model\n",
    "entity_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "MODEL = \"deepseek-r1:8b-llama-distill-q8_0\"\n",
    "\n",
    "file_path =\"PDFs/sensors.pdf\"\n",
    "\n",
    "file_name = file_path.split('/')[-1].split('.')[0]\n",
    "response_path = 'responses/R1_responses_{file}.json'.format(file=file_name)\n",
    "generated_path = 'generated_JSON/R1_generated_{file}.json'.format(file=file_name)\n",
    "ontology_path = 'ontology/R1_ontology_{file}.owl'.format(file=file_name)\n",
    "validtion_json_path = 'QA/R1_validation_{file}.json'.format(file=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove LaTeX equations\n",
    "    text = re.sub(r'\\$.*?\\$', '', text)  # Remove inline equations\n",
    "    # Fix hyphenated words\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "    # Remove excessive whitespace\n",
    "    return re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF\n",
    "pdf_loader = PyPDFLoader(file_path=file_path,extract_images=False)\n",
    "docs = pdf_loader.load();\n",
    "# Enhanced cleaning pipeline\n",
    "for doc in docs:\n",
    "    doc.page_content = clean_text(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_splits(splits):\n",
    "    stats = {\n",
    "        'total_chunks': len(splits),\n",
    "        'avg_chunk_length': sum(len(c.page_content) for c in splits)/len(splits),\n",
    "        'max_length': max(len(c.page_content) for c in splits),\n",
    "        'min_length': min(len(c.page_content) for c in splits),\n",
    "        'metadata_fields': list(splits[0].metadata.keys()) if splits else []\n",
    "    }\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_chunks': 30,\n",
       " 'avg_chunk_length': 1872.0,\n",
       " 'max_length': 3228,\n",
       " 'min_length': 352,\n",
       " 'metadata_fields': ['producer',\n",
       "  'creator',\n",
       "  'creationdate',\n",
       "  'author',\n",
       "  'keywords',\n",
       "  'moddate',\n",
       "  'subject',\n",
       "  'title',\n",
       "  'source',\n",
       "  'total_pages',\n",
       "  'page',\n",
       "  'page_label',\n",
       "  'start_index']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        length_function=lambda text: len(text.split()),  # Word-based counting\n",
    "        add_start_index=True,\n",
    "       separators=[\n",
    "        \"\\n\\n## \",    # Section headers\n",
    "        \"\\n\\n\",       # Paragraph breaks\n",
    "        \"\\n\",         # New lines\n",
    "        \"(?<!\\d)\\.(?!\\d)\\s+\",  # Sentence ends with space\n",
    "        \";\",          # Semi-colons\n",
    "        \", \",         # Commas\n",
    "        \" \"\n",
    "        ],\n",
    "        keep_separator=True,\n",
    "        is_separator_regex=True,\n",
    "    )\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "analyze_splits(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_24032\\622889800.py:2: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=MODEL)\n",
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_24032\\622889800.py:19: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=MODEL)\n",
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_24032\\622889800.py:30: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = mq_retriever.get_relevant_documents(combined_query)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Store Embeddings in FAISS (Faster Retrieval)\n",
    "embeddings = OllamaEmbeddings(model=MODEL)\n",
    "faiss_store = FAISS.from_documents(splits, embeddings)\n",
    "\n",
    "# Step 2: Set Up Retrieval Methods\n",
    "vector_retriever = faiss_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "bm25_retriever = BM25Retriever.from_documents(splits)\n",
    "bm25_retriever.k = 3  # Match vector retriever\n",
    "\n",
    "# Dynamic Weights for Different Queries\n",
    "query_type = \"entity\"  # or \"relation\"\n",
    "weights = [0.8, 0.2] if query_type == \"entity\" else [0.6, 0.4]\n",
    "\n",
    "# Ensemble Retriever\n",
    "retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever],\n",
    "    weights=weights\n",
    ")\n",
    "llm = ChatOllama(model=MODEL) \n",
    "# Multi-Query Expansion (Better Recall)\n",
    "mq_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)\n",
    "\n",
    "# Retrieve Once, Then Filter\n",
    "combined_query = \"\"\"\n",
    "EXTRACT: LiDAR sensors, their components, technical specifications.\n",
    "FIND: has_part, implements, measurement_properties relationships.\n",
    "IGNORE: Experimental results, methodology, figures.\n",
    "FILTER: Only technical specifications sections.\n",
    "\"\"\"\n",
    "retrieved_docs = mq_retriever.get_relevant_documents(combined_query)\n",
    "\n",
    "retrieved_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with your local model\n",
    "compressor = LLMChainExtractor.from_llm(llm=llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever,\n",
    "    search_kwargs={\"k\": 10}  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "KG_EXTRACTION_PROMPT=\"\"\"As a LiDAR sensor expert, analyze this technical text to extract:\n",
    "\n",
    "**Target Entities**:\n",
    "1. Sensor Models: Manufacturer-branded names (Velodyne HDL-64E, Livox Horizon)\n",
    "2. Components: Physical/software parts (e.g., MEMS mirror, FPGA processor)\n",
    "3. Technical Specs: Quantified values with units (120m range, 0.08° resolution)\n",
    "4. Implementations: Standards/protocols (IEEE 802.11p, ROS2)\n",
    "5. Category: Automotive LiDAR, Industrial LiDAR, etc.\n",
    "\n",
    "**Extraction Rules**:\n",
    "- Preserve contextual relationships: \n",
    "  *Example*:\n",
    "  \"The Velodyne VLP-32C rotating assembly enables 360° coverage\" → \n",
    "  {{\"parts\": [\"rotating assembly\"], \"properties\": [\"field of view: 360°\"]}}\n",
    "- Capture implied properties from comparisons:\n",
    "  *Example*:\n",
    "  \"Outperforms Ouster OS2 in range\" → \n",
    "  {{\"properties\": [\"comparative_range: > Ouster OS2\"]}}\n",
    "\n",
    "\n",
    "**IMPORTANT**:\n",
    "The example block is for demonstration purposes only and should not affect the final extraction.\n",
    "Do NOT include any sensor data from the example in your final output.\n",
    "Only extract sensors that are explicitly mentioned in the text below.\n",
    "\n",
    "<example>\n",
    "Input: \"The Velodyne VLP-32C has a 200m range and 0.2° resolution\"\n",
    "Output:\n",
    "{{\n",
    "  \"sensors\": [\n",
    "    {{\n",
    "      \"name\": \"Velodyne VLP-32C\",\n",
    "      \"category\": \"Automotive LiDAR\",\n",
    "      \"parts\": [],\n",
    "      \"properties\": [\"range: 200m\", \"resolution: 0.2°\"],\n",
    "      \"implements\": []\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "</example>\n",
    "\n",
    "**Format**:\n",
    "```json\n",
    "{{\n",
    "  \"sensors\": [\n",
    "    {{\n",
    "      \"name\": \"\",\n",
    "      \"category\": \"\",\n",
    "      \"parts\": [],\n",
    "      \"properties\": [],\n",
    "      \"implements\": []\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "```\n",
    "**Critical Instructions**:\n",
    "1. If no sensors are found, return: {{\"sensors\": []}}\n",
    "2. Always maintain valid JSON structure\n",
    "3. Never add extra text outside the JSON\n",
    "4. Use exact values from tables when available\n",
    "\n",
    "<text>\n",
    "{text}\n",
    "<text>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseFormat(Enum):\n",
    "    JSON = \"json_object\"\n",
    "    TEXT = \"text\"\n",
    " \n",
    " \n",
    "def call_model(\n",
    "    prompt: str, response_format: ResponseFormat = ResponseFormat.TEXT\n",
    ") -> str:\n",
    "    response = ollama.generate(\n",
    "        model=MODEL,\n",
    "        prompt=prompt,\n",
    "        keep_alive=\"1h\",\n",
    "        format=\"\" if response_format == ResponseFormat.TEXT else \"json\",\n",
    "    )\n",
    "    return response[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(retrieved_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the final prompt with the concatenated text\n",
    "\n",
    "responses = []\n",
    "\n",
    "for chunk in chunks:\n",
    "        final_prompt = KG_EXTRACTION_PROMPT.format(text=chunk)\n",
    "        # Send the final prompt to Ollama\n",
    "        # print (final_prompt)\n",
    "        response = call_model(final_prompt)\n",
    "        responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Save responses after every chunk to ensure progress is retained\n",
    "with open(response_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(responses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_string(s):\n",
    "    \"\"\"\n",
    "    Clean and normalize strings by stripping whitespace, decoding unicode escapes,\n",
    "    normalizing characters, replacing common separators with a space, and fixing known encoding issues.\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return str(s).lower()\n",
    "    s = s.strip()\n",
    "    s = unicodedata.normalize('NFKC', s)\n",
    "    s = s.replace(\"â\", \"\")\n",
    "    s = s.replace('<', ',')\n",
    "    s = s.replace('>', ',')\n",
    "    return s.lower()\n",
    "\n",
    "def make_valid_iri_fragment(s):\n",
    "    \"\"\"\n",
    "    Convert a string into a valid IRI fragment:\n",
    "    - First clean the string,\n",
    "    - Then use URL encoding to percent-encode any characters not in the safe set.\n",
    "    \"\"\"\n",
    "    cleaned = clean_string(s)\n",
    "    # Allow only alphanumerics, underscore, and hyphen.\n",
    "    safe_chars = \"_-abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n",
    "    return urllib.parse.quote(cleaned, safe=safe_chars)\n",
    "\n",
    "def extract_json_part(response_str):\n",
    "    \"\"\"\n",
    "    Extract the JSON block from Ollama's response string.\n",
    "    Tries to find the JSON block delimited by ```json and ```.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_str = response_str.split(\"```json\")[-1].split(\"```\")[0].strip()\n",
    "\n",
    "        return json.loads(json_str)\n",
    "    except (IndexError, json.JSONDecodeError) as e:\n",
    "        print(f\"Failed to extract JSON: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load responses from file\n",
    "with open(response_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = f.read()\n",
    "\n",
    "# Try parsing as a full JSON array; if that fails, split by lines.\n",
    "try:\n",
    "    responses = json.loads(raw_data)\n",
    "except json.JSONDecodeError:\n",
    "    responses = [json.loads(line) for line in raw_data.splitlines() if line.strip()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 29 unique sensors.\n"
     ]
    }
   ],
   "source": [
    "# Use a dictionary to accumulate unique sensors based on (name, category, parts, properties)\n",
    "sensor_dict = {}\n",
    "\n",
    "for response in responses:\n",
    "    # If the response is not a dict, try extracting the JSON block\n",
    "    if not isinstance(response, dict):\n",
    "        extracted = extract_json_part(response)\n",
    "        if extracted is None:\n",
    "            continue\n",
    "        response = extracted\n",
    "\n",
    "    if isinstance(response, list):\n",
    "        continue\n",
    "    sensors = response.get(\"sensors\", [])\n",
    "    if not isinstance(sensors, list):\n",
    "        continue\n",
    "\n",
    "    for sensor in sensors:\n",
    "        # Validate required fields and clean them\n",
    "        sensor_name = clean_string(sensor.get(\"name\", \"unnamed sensor\"))\n",
    "        category_name = clean_string(sensor.get(\"category\", \"uncategorized\"))\n",
    "        parts_list = sensor.get(\"parts\", [])\n",
    "        properties_list = sensor.get(\"properties\", [])\n",
    "        implements_list = sensor.get(\"implements\", [])\n",
    "\n",
    "        cleaned_parts = sorted(clean_string(p) for p in parts_list if p)\n",
    "        cleaned_properties = sorted(clean_string(p) for p in properties_list if p)\n",
    "        cleaned_implements = sorted(clean_string(i) for i in implements_list if i)\n",
    "\n",
    "        # Use key without implements for merging duplicates:\n",
    "        key = (sensor_name, category_name, tuple(cleaned_parts), tuple(cleaned_properties))\n",
    "        \n",
    "        if key in sensor_dict:\n",
    "            # Merge the implements list (union of values)\n",
    "            current_impl = set(sensor_dict[key][\"implements\"])\n",
    "            new_impl = set(cleaned_implements)\n",
    "            sensor_dict[key][\"implements\"] = sorted(current_impl.union(new_impl))\n",
    "        else:\n",
    "            sensor_dict[key] = {\n",
    "                \"name\": sensor_name,\n",
    "                \"category\": category_name,\n",
    "                \"parts\": sorted(cleaned_parts),\n",
    "                \"properties\": sorted(cleaned_properties),\n",
    "                \"implements\": sorted(cleaned_implements)\n",
    "            }\n",
    "\n",
    "# Convert the sensor dictionary to a list\n",
    "final_sensors = list(sensor_dict.values())\n",
    "\n",
    "# Save the final, deduplicated JSON\n",
    "with open(generated_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    json.dump({\"sensors\": final_sensors}, out_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Processed {len(final_sensors)} unique sensors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sensor: 'velarray' in category 'automotive lidar'\n",
      "Processing sensor: 'blickfeld cube' in category 'automotive lidar'\n",
      "Processing sensor: 'velodyne vlp-32c' in category 'automotive lidar'\n",
      "Processing sensor: 'velodyne vlp-32c' in category 'automotive lidar'\n",
      "Processing sensor: 'ouster os2' in category 'automotive lidar'\n",
      "Processing sensor: 'livox horizon' in category 'automotive lidar'\n",
      "Processing sensor: '' in category ''\n",
      "Skipping sensor '' because it equals its category.\n",
      "Processing sensor: '' in category 'automotive lidar'\n",
      "Processing sensor: '360 ° rotary lidar' in category 'automotive lidar'\n",
      "Processing sensor: 'robosense m1' in category 'automotive lidar'\n",
      "Processing sensor: 'velodyne velarray h800' in category 'automotive lidar'\n",
      "Processing sensor: 'livox horizon' in category 'automotive lidar'\n",
      "Processing sensor: '' in category 'generic lidar'\n",
      "Processing sensor: 'velodyne velarray h800' in category 'automotive lidar'\n",
      "Processing sensor: 'livox horizon' in category 'automotive lidar'\n",
      "Processing sensor: '' in category 'automotive lidar'\n",
      "Processing sensor: '' in category 'automotive lidar'\n",
      "Processing sensor: '' in category 'lidar sensors'\n",
      "Processing sensor: 'velodyne hdl-64e' in category 'automotive lidar'\n",
      "Processing sensor: 'livox horizon' in category 'automotive lidar'\n",
      "Processing sensor: 'blickfeld cube' in category 'automotive lidar'\n",
      "Processing sensor: 'blickfeld cube range' in category 'automotive lidar'\n",
      "Processing sensor: 'innoviz pro' in category 'automotive lidar'\n",
      "Processing sensor: 'velodyne' in category 'automotive lidar'\n",
      "Processing sensor: 'livox' in category 'automotive lidar'\n",
      "Processing sensor: 'blickfeld' in category 'automotive lidar'\n",
      "Processing sensor: 'horizon m1 cube' in category 'automotive lidar'\n",
      "Processing sensor: 'velarray h800 pro' in category 'automotive lidar'\n",
      "Processing sensor: 'lidar' in category 'automotive lidar'\n",
      "Ontology created and saved as sensor_ontology_with_properties_and_implements.owl\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON file\n",
    "with open(generated_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a new ontology\n",
    "onto = get_ontology(\"http://example.org/sensor_ontology.owl\")\n",
    "\n",
    "with onto:\n",
    "    # Define basic classes and properties\n",
    "    class Sensor(Thing):\n",
    "        pass\n",
    "    \n",
    "    class Part(Thing):\n",
    "        pass\n",
    "    \n",
    "    class Property(Thing):\n",
    "        pass\n",
    "    \n",
    "    class Technology(Thing):\n",
    "        pass\n",
    "\n",
    "    class has_part_directly(ObjectProperty):\n",
    "        domain = [Sensor]\n",
    "        range = [Part]\n",
    "    \n",
    "    class implements(ObjectProperty):\n",
    "        domain = [Sensor]\n",
    "        range = [Technology]\n",
    "    \n",
    "    class has_property(ObjectProperty):\n",
    "        domain = [Sensor]\n",
    "        range = [Property]\n",
    "\n",
    "    # Define an annotation property for category\n",
    "    class category(AnnotationProperty):\n",
    "        pass\n",
    "\n",
    "    # Process the JSON data\n",
    "    category_classes = {} # dictionary to store created category classes.\n",
    "    part_classes = {}  # Cache for part classes to avoid duplicates\n",
    "    property_classes = {}  # Cache for property classes\n",
    "    technology_classes = {}  # Cache for technology classes\n",
    "\n",
    "# Process each sensor from the JSON data\n",
    "    for sensor_data in data[\"sensors\"]:\n",
    "        # Clean sensor name and category\n",
    "        # sensor_name = clean_string(sensor_data[\"name\"]).replace(\" \", \"_\")\n",
    "        sensor_name = sensor_data[\"name\"]\n",
    "        # sensor_category_name = clean_string(sensor_data[\"category\"]).replace(\" \", \"_\")\n",
    "        sensor_category_name = sensor_data[\"category\"]\n",
    "        # Avoid creating a sensor if its name equals the category (to prevent a cycle)\n",
    "        if sensor_name == sensor_category_name:\n",
    "            print(f\"Skipping sensor '{sensor_name}' because it equals its category.\")\n",
    "            continue\n",
    "        \n",
    "        # Create or retrieve the category class (as a subclass of Sensor)\n",
    "        if sensor_category_name not in category_classes:\n",
    "            # Create new category class as a subclass of Sensor\n",
    "            category_class = types.new_class(sensor_category_name, (Sensor,))\n",
    "            category_classes[sensor_category_name] = category_class\n",
    "        else:\n",
    "            category_class = category_classes[sensor_category_name]\n",
    "\n",
    "        # Create the sensor model as a subclass of the category class\n",
    "        sensor_class = types.new_class(sensor_name, (category_class,))\n",
    "        \n",
    "        # Process and link parts\n",
    "        for part_name in sensor_data.get(\"parts\", []):\n",
    "            # part_clean = clean_string(part_name).replace(\" \", \"_\")\n",
    "            part_clean = make_valid_iri_fragment(part_name)\n",
    "            if part_clean not in part_classes:\n",
    "                part_class = types.new_class(part_clean, (Part,))\n",
    "                part_classes[part_clean] = part_class\n",
    "            else:\n",
    "                part_class = part_classes[part_clean]\n",
    "            sensor_class.is_a.append(has_part_directly.some(part_class))\n",
    "\n",
    "        # Process and link properties\n",
    "        for prop_name in sensor_data.get(\"properties\", []):\n",
    "            # prop_clean = clean_string(prop_name).replace(\" \", \"_\")\n",
    "            prop_clean = make_valid_iri_fragment(prop_name)\n",
    "            if prop_clean not in property_classes:\n",
    "                prop_class = types.new_class(prop_clean, (Property,))\n",
    "                property_classes[prop_clean] = prop_class\n",
    "            else:\n",
    "                prop_class = property_classes[prop_clean]\n",
    "            sensor_class.is_a.append(has_property.some(prop_class))\n",
    "\n",
    "        # Process and link technologies (implements relationship)\n",
    "        for tech_name in sensor_data.get(\"implements\", []):\n",
    "            # tech_clean = clean_string(tech_name).replace(\" \", \"_\")\n",
    "            tech_clean = make_valid_iri_fragment(tech_name)\n",
    "            if tech_clean not in technology_classes:\n",
    "                tech_class = types.new_class(tech_clean, (Technology,))\n",
    "                technology_classes[tech_clean] = tech_class\n",
    "            else:\n",
    "                tech_class = technology_classes[tech_clean]\n",
    "            sensor_class.is_a.append(implements.some(tech_class))\n",
    "# Save the ontology to a file\n",
    "onto.save(file=ontology_path, format=\"rdfxml\")\n",
    "\n",
    "print(\"Ontology created and saved as sensor_ontology_with_properties_and_implements.owl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzzy verification function using fuzzywuzzy\n",
    "def fuzzy_verify(name, chunks, threshold=75):\n",
    "    name_clean = name.lower()\n",
    "    max_score = 0\n",
    "    for chunk in chunks:\n",
    "        chunk_clean = chunk.page_content.lower()\n",
    "        score = fuzz.token_set_ratio(name_clean, chunk_clean)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "    return max_score, max_score >= threshold\n",
    "\n",
    "# Semantic verification using sentence-transformers\n",
    "def semantic_verify(sensor, chunks, model, threshold=0.65):\n",
    "    # Use sensor name and category for additional context\n",
    "    category = sensor.get('category', '').strip()\n",
    "    name = sensor.get('name', '').strip()\n",
    "    if category:\n",
    "        query_text = f\"LiDAR sensor model {name} used in {category}\"\n",
    "    else:\n",
    "        query_text = f\"LiDAR sensor model {name}\"\n",
    "        \n",
    "    query_embedding = model.encode(query_text)\n",
    "    \n",
    "    # Encode each chunk and compute cosine similarities\n",
    "    chunk_embeddings = [model.encode(chunk.page_content) for chunk in chunks]\n",
    "    similarities = cosine_similarity([query_embedding], chunk_embeddings)[0]\n",
    "    max_sim = float(np.max(similarities))\n",
    "    return max_sim, max_sim >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(generated_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    sensor_data = json.load(f)\n",
    "\n",
    "# Iterate through sensors and validate against the PDF chunks\n",
    "qa_results = []\n",
    "for sensor in sensor_data.get(\"sensors\", []):\n",
    "    name = sensor.get(\"name\", \"\")\n",
    "    \n",
    "    # Exact string search (case-insensitive)\n",
    "    string_found = any(name.lower() in chunk.lower() for chunk in chunks)\n",
    "    \n",
    "    # Fuzzy matching remains unchanged\n",
    "    fuzzy_score, fuzzy_match = fuzzy_verify(name, splits, threshold=80)\n",
    "    \n",
    "    # Use the updated semantic_verify function with contextual query text\n",
    "    semantic_score, semantic_match = semantic_verify(sensor, splits, entity_model, threshold=0.65)\n",
    "    \n",
    "    # Compute a weighted confidence score\n",
    "    confidence = (string_found * 0.4) + (fuzzy_score / 100 * 0.3) + (semantic_score * 0.3)\n",
    "    \n",
    "    qa_results.append({\n",
    "        \"sensor\": name,\n",
    "        \"string_match\": string_found,\n",
    "        \"fuzzy_score\": fuzzy_score,\n",
    "        \"fuzzy_match\": fuzzy_match,\n",
    "        \"semantic_score\": semantic_score,\n",
    "        \"semantic_match\": semantic_match,\n",
    "        \"confidence\": confidence\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(validtion_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qa_results, f,indent=2)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
