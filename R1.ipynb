{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -Uqqq pip --progress-bar off\n",
    "# %pip install -qqq ollama --progress-bar off\n",
    "\n",
    "\n",
    "# %pip install -qqq owlready2 --progress-bar off\n",
    "\n",
    "# %pip install -qqq langchain-ollama --progress-bar off\n",
    "# %pip install -qqq langchain-community --progress-bar off\n",
    "# %pip install -qqq pypdf --progress-bar off\n",
    "\n",
    "# %pip install -qqq faiss-cpu --progress-bar off\n",
    "# %pip install -qqq rank_bm25 --progress-bar off\n",
    "# %pip install -qqq fuzzywuzzy --progress-bar off\n",
    "# %pip install -qqq scikit-learn --progress-bar off\n",
    "# %pip install -qqq sentence-transformers --progress-bar off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohit\\Documents\\Rohit\\Dev\\Thesis\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\rohit\\Documents\\Rohit\\Dev\\Thesis\\.venv\\Lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import ollama\n",
    "import json\n",
    "from enum import Enum\n",
    "import json\n",
    "import re\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.chat_models import ChatOllama\n",
    "from owlready2 import *\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from fuzzywuzzy import fuzz\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers import EnsembleRetriever, BM25Retriever, MultiQueryRetriever\n",
    "from langchain.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SentenceTransformer model\n",
    "entity_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "MODEL = \"deepseek-r1:8b-llama-distill-q8_0\"\n",
    "\n",
    "file_path =\"PDFs/sensors.pdf\"\n",
    "\n",
    "file_name = file_path.split('/')[-1].split('.')[0]\n",
    "response_path = 'responses/R1_temp_responses_{file}.json'.format(file=file_name)\n",
    "generated_path = 'generated_JSON/R1_temp_generated_{file}.json'.format(file=file_name)\n",
    "ontology_path = 'ontology/R1_temp_ontology_{file}.owl'.format(file=file_name)\n",
    "validation_json_path = 'QA/R1_temp_validation_{file}.json'.format(file=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove LaTeX equations\n",
    "    text = re.sub(r'\\$.*?\\$', '', text)  # Remove inline equations\n",
    "    # Fix hyphenated words\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "    # Remove excessive whitespace\n",
    "    return re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF\n",
    "pdf_loader = PyPDFLoader(file_path=file_path,extract_images=False)\n",
    "docs = pdf_loader.load();\n",
    "# Enhanced cleaning pipeline\n",
    "for doc in docs:\n",
    "    doc.page_content = clean_text(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_splits(splits):\n",
    "    stats = {\n",
    "        'total_chunks': len(splits),\n",
    "        'avg_chunk_length': sum(len(c.page_content) for c in splits)/len(splits),\n",
    "        'max_length': max(len(c.page_content) for c in splits),\n",
    "        'min_length': min(len(c.page_content) for c in splits),\n",
    "        'metadata_fields': list(splits[0].metadata.keys()) if splits else []\n",
    "    }\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'total_chunks': 30,\n",
       " 'avg_chunk_length': 1872.0,\n",
       " 'max_length': 3228,\n",
       " 'min_length': 352,\n",
       " 'metadata_fields': ['producer',\n",
       "  'creator',\n",
       "  'creationdate',\n",
       "  'author',\n",
       "  'keywords',\n",
       "  'moddate',\n",
       "  'subject',\n",
       "  'title',\n",
       "  'source',\n",
       "  'total_pages',\n",
       "  'page',\n",
       "  'page_label',\n",
       "  'start_index']}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        length_function=lambda text: len(text.split()),  # Word-based counting\n",
    "        add_start_index=True,\n",
    "       separators=[\n",
    "        \"\\n\\n## \",    # Section headers\n",
    "        \"\\n\\n\",       # Paragraph breaks\n",
    "        \"\\n\",         # New lines\n",
    "        \"(?<!\\d)\\.(?!\\d)\\s+\",  # Sentence ends with space\n",
    "        \";\",          # Semi-colons\n",
    "        \", \",         # Commas\n",
    "        \" \"\n",
    "        ],\n",
    "        keep_separator=True,\n",
    "        is_separator_regex=True,\n",
    "    )\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "analyze_splits(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_30676\\622889800.py:2: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=MODEL)\n",
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_30676\\622889800.py:19: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=MODEL)\n",
      "C:\\Users\\rohit\\AppData\\Local\\Temp\\ipykernel_30676\\622889800.py:30: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  retrieved_docs = mq_retriever.get_relevant_documents(combined_query)\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Store Embeddings in FAISS (Faster Retrieval)\n",
    "embeddings = OllamaEmbeddings(model=MODEL)\n",
    "faiss_store = FAISS.from_documents(splits, embeddings)\n",
    "\n",
    "# Step 2: Set Up Retrieval Methods\n",
    "vector_retriever = faiss_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "bm25_retriever = BM25Retriever.from_documents(splits)\n",
    "bm25_retriever.k = 3  # Match vector retriever\n",
    "\n",
    "# Dynamic Weights for Different Queries\n",
    "query_type = \"entity\"  # or \"relation\"\n",
    "weights = [0.8, 0.2] if query_type == \"entity\" else [0.6, 0.4]\n",
    "\n",
    "# Ensemble Retriever\n",
    "retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever],\n",
    "    weights=weights\n",
    ")\n",
    "llm = ChatOllama(model=MODEL) \n",
    "# Multi-Query Expansion (Better Recall)\n",
    "mq_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)\n",
    "\n",
    "# Retrieve Once, Then Filter\n",
    "combined_query = \"\"\"\n",
    "EXTRACT: LiDAR sensors, their components, technical specifications.\n",
    "FIND: has_part, implements, measurement_properties relationships.\n",
    "IGNORE: Experimental results, methodology, figures.\n",
    "FILTER: Only technical specifications sections.\n",
    "\"\"\"\n",
    "retrieved_docs = mq_retriever.get_relevant_documents(combined_query)\n",
    "\n",
    "retrieved_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with your local model\n",
    "compressor = LLMChainExtractor.from_llm(llm=llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever,\n",
    "    search_kwargs={\"k\": 10}  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "KG_EXTRACTION_PROMPT = \"\"\"\n",
    "As a LiDAR sensor expert, analyze this technical text to extract:\n",
    "\n",
    "**Target Entities**:\n",
    "1. Sensor Models: Manufacturer-branded names (e.g., Velodyne HDL-64E, Livox Horizon)\n",
    "2. Components: Physical/software parts (e.g., MEMS mirror, FPGA processor)\n",
    "3. Technical Specs: Quantified values with units (e.g., 120m range, 0.08° resolution)\n",
    "4. Implementations: Standards/protocols (e.g., IEEE 802.11p, ROS2)\n",
    "5. Category: Automotive LiDAR, Industrial LiDAR, etc.\n",
    "6. Devices: The specific equipment or systems these sensors are part of (e.g., autonomous vehicles, drones)\n",
    "\n",
    "**Extraction Rules**:\n",
    "- Preserve contextual relationships:\n",
    "  *Example for Guidance Only (Do NOT include this data in your output)*: \"The Velodyne VLP-32C rotating assembly enables 360° coverage\" → {{\"parts\": [\"rotating assembly\"], \"properties\": [\"field of view: 360°\"]}}\n",
    "- Capture implied properties from comparisons:\n",
    "  *Example for Guidance Only (Do NOT include this data in your output)*: \"Outperforms Ouster OS2 in range\" → {{\"properties\": [\"comparative_range: > Ouster OS2\"]}}\n",
    "- Identify devices associated with sensors and link them appropriately.\n",
    "\n",
    "**Example for Guidance Only (Do NOT include this data in your output)**:\n",
    "*Input*: \"The Velodyne VLP-32C is used in the Tesla Cybertruck for LiDAR sensing.\"\n",
    "*Output*: \n",
    "{{\n",
    "  \"sensors\": [\n",
    "    {{\n",
    "      \"name\": \"Velodyne VLP-32C\",\n",
    "      \"category\": \"Automotive LiDAR\",\n",
    "      \"parts\": [],\n",
    "      \"properties\": [\"range: 200m\", \"resolution: 0.2°\"],\n",
    "      \"implements\": [],\n",
    "      \"device\": \"Tesla Cybertruck\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "**Format**:\n",
    "{{\n",
    "  \"sensors\": [\n",
    "    {{\n",
    "      \"name\": \"\",\n",
    "      \"category\": \"\",\n",
    "      \"parts\": [],\n",
    "      \"properties\": [],\n",
    "      \"implements\": [],\n",
    "      \"device\": \"\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "**Critical Instructions**:\n",
    "1. If no sensors are found, return: {{\"sensors\": []}}\n",
    "2. Only include sensor information that is explicitly mentioned in the provided text.\n",
    "3. Do not include any sensor names, components, properties, implementations, or devices from the guidance example.\n",
    "4. Do not add any extra text or commentary outside the JSON output.\n",
    "5. Use exact values from the text when available.\n",
    "\n",
    "<text> {text} </text>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseFormat(Enum):\n",
    "    JSON = \"json_object\"\n",
    "    TEXT = \"text\"\n",
    " \n",
    " \n",
    "def call_model(\n",
    "    prompt: str, response_format: ResponseFormat = ResponseFormat.TEXT\n",
    ") -> str:\n",
    "    response = ollama.generate(\n",
    "        model=MODEL,\n",
    "        prompt=prompt,\n",
    "        keep_alive=\"1h\",\n",
    "        format=\"\" if response_format == ResponseFormat.TEXT else \"json\",\n",
    "    )\n",
    "    return response[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(retrieved_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the final prompt with the concatenated text\n",
    "\n",
    "responses = []\n",
    "\n",
    "for chunk in chunks:\n",
    "        final_prompt = KG_EXTRACTION_PROMPT.format(text=chunk)\n",
    "        # Send the final prompt to Ollama\n",
    "        # print (final_prompt)\n",
    "        response = call_model(final_prompt)\n",
    "        responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Save responses after every chunk to ensure progress is retained\n",
    "with open(response_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(responses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_string(s):\n",
    "    \"\"\"\n",
    "    Clean and normalize strings by stripping whitespace, decoding unicode escapes,\n",
    "    normalizing characters, replacing common separators with a space, and fixing known encoding issues.\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return str(s).lower()\n",
    "    s = s.strip()\n",
    "    s = unicodedata.normalize('NFKC', s)\n",
    "    s = s.replace(\"â\", \"\")\n",
    "    s = s.replace('<', ',')\n",
    "    s = s.replace('>', ',')\n",
    "    return s.lower()\n",
    "\n",
    "def make_valid_iri_fragment(s):\n",
    "    \"\"\"\n",
    "    Convert a string into a valid IRI fragment:\n",
    "    - First clean the string,\n",
    "    - Then use URL encoding to percent-encode any characters not in the safe set.\n",
    "    \"\"\"\n",
    "    cleaned = clean_string(s)\n",
    "    # Allow only alphanumerics, underscore, and hyphen.\n",
    "    safe_chars = \"_-abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\"\n",
    "    return urllib.parse.quote(cleaned, safe=safe_chars)\n",
    "\n",
    "def extract_json_part(response_str):\n",
    "    \"\"\"\n",
    "    Extract the JSON block from Ollama's response string.\n",
    "    Tries to find the JSON block delimited by ```json and ```.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_str = response_str.split(\"```json\")[-1].split(\"```\")[0].strip()\n",
    "\n",
    "        return json.loads(json_str)\n",
    "    except (IndexError, json.JSONDecodeError) as e:\n",
    "        print(f\"Failed to extract JSON: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load responses from file\n",
    "with open(response_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = f.read()\n",
    "\n",
    "# Try parsing as a full JSON array; if that fails, split by lines.\n",
    "try:\n",
    "    responses = json.loads(raw_data)\n",
    "except json.JSONDecodeError:\n",
    "    responses = [json.loads(line) for line in raw_data.splitlines() if line.strip()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to extract JSON: Expecting value: line 1 column 1 (char 0)\n",
      "Failed to extract JSON: Expecting value: line 1 column 1 (char 0)\n",
      "Failed to extract JSON: Expecting value: line 1 column 1 (char 0)\n",
      "Failed to extract JSON: Expecting value: line 1 column 1 (char 0)\n",
      "Failed to extract JSON: Expecting value: line 1 column 1 (char 0)\n",
      "Processed 9 unique sensors.\n"
     ]
    }
   ],
   "source": [
    "# Use a dictionary keyed solely on sensor name\n",
    "sensor_dict = {}\n",
    "\n",
    "for response in responses:\n",
    "    # If the response is not a dict, try extracting the JSON block\n",
    "    if not isinstance(response, dict):\n",
    "        extracted = extract_json_part(response)\n",
    "        if extracted is None:\n",
    "            continue\n",
    "        response = extracted\n",
    "\n",
    "    if isinstance(response, list):\n",
    "        continue\n",
    "    sensors = response.get(\"sensors\", [])\n",
    "    if not isinstance(sensors, list):\n",
    "        continue\n",
    "\n",
    "    for sensor in sensors:\n",
    "        # Clean and extract fields from the sensor entry\n",
    "        sensor_name = clean_string(sensor.get(\"name\", \"unnamed sensor\"))\n",
    "        if not sensor_name:\n",
    "            continue\n",
    "        category_name = clean_string(sensor.get(\"category\", \"uncategorized\"))\n",
    "        parts_list = sensor.get(\"parts\", [])\n",
    "        properties_list = sensor.get(\"properties\", [])\n",
    "        implements_list = sensor.get(\"implements\", [])\n",
    "        # Note: if your field is \"devices\" (plural), adjust accordingly\n",
    "        device_info = clean_string(sensor.get(\"device\", \"\")) or \"\"\n",
    "        \n",
    "        cleaned_parts = set(clean_string(p) for p in parts_list if p)\n",
    "        cleaned_properties = set(clean_string(p) for p in properties_list if p)\n",
    "        cleaned_implements = set(clean_string(i) for i in implements_list if i)\n",
    "        devices = set()\n",
    "        if device_info:\n",
    "            devices.add(device_info)\n",
    "        \n",
    "        # Use sensor_name as the key for merging\n",
    "        key = sensor_name\n",
    "        \n",
    "        if key in sensor_dict:\n",
    "            sensor_dict[key][\"category\"].add(category_name)\n",
    "            sensor_dict[key][\"parts\"].update(cleaned_parts)\n",
    "            sensor_dict[key][\"properties\"].update(cleaned_properties)\n",
    "            sensor_dict[key][\"implements\"].update(cleaned_implements)\n",
    "            sensor_dict[key][\"devices\"].update(devices)\n",
    "        else:\n",
    "            sensor_dict[key] = {\n",
    "                \"name\": sensor_name,\n",
    "                \"category\": {category_name},\n",
    "                \"parts\": cleaned_parts,\n",
    "                \"properties\": cleaned_properties,\n",
    "                \"implements\": cleaned_implements,\n",
    "                \"devices\": devices\n",
    "            }\n",
    "\n",
    "# Convert sets to sorted lists and merge categories into a single string\n",
    "final_sensors = []\n",
    "for sensor in sensor_dict.values():\n",
    "    sensor[\"category\"] = \", \".join(sorted(sensor[\"category\"]))\n",
    "    sensor[\"parts\"] = sorted(sensor[\"parts\"])\n",
    "    sensor[\"properties\"] = sorted(sensor[\"properties\"])\n",
    "    sensor[\"implements\"] = sorted(sensor[\"implements\"])\n",
    "    sensor[\"devices\"] = sorted(sensor[\"devices\"])\n",
    "    final_sensors.append(sensor)\n",
    "\n",
    "# Save the deduplicated JSON\n",
    "with open(generated_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    json.dump({\"sensors\": final_sensors}, out_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Processed {len(final_sensors)} unique sensors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ontology created and saved as sensor_ontology_with_properties_and_implements.owl\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON file\n",
    "with open(generated_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a new ontology\n",
    "onto = get_ontology(\"http://example.org/sensor_ontology.owl\")\n",
    "\n",
    "# Create a new ontology\n",
    "onto = get_ontology(\"http://example.org/sensor_ontology.owl\")\n",
    "\n",
    "# Create a new ontology\n",
    "onto = get_ontology(\"http://example.org/sensor_ontology.owl\")\n",
    "\n",
    "with onto:\n",
    "    # Define base classes\n",
    "    class Device(Thing):\n",
    "        pass\n",
    "\n",
    "    # A new class representing a sensor component (i.e., a sensor category within a device)\n",
    "    class SensorComponent(Thing):\n",
    "        pass\n",
    "\n",
    "    # We may keep a general Sensor class if needed\n",
    "    class Sensor(Thing):\n",
    "        pass\n",
    "    \n",
    "    class Part(Thing):\n",
    "        pass\n",
    "    \n",
    "    class Property(Thing):\n",
    "        pass\n",
    "    \n",
    "    class Technology(Thing):\n",
    "        pass\n",
    "\n",
    "    # Define object properties\n",
    "    class has_part_directly(ObjectProperty):\n",
    "        domain = [Sensor]\n",
    "        range = [Part]\n",
    "    \n",
    "    class has_property(ObjectProperty):\n",
    "        domain = [Sensor]\n",
    "        range = [Property]\n",
    "    \n",
    "    class implements(ObjectProperty):\n",
    "        domain = [Sensor]\n",
    "        range = [Technology]\n",
    "    \n",
    "    # New property to link sensors to devices, if needed.\n",
    "    class used_in(ObjectProperty):\n",
    "        domain = [Sensor]\n",
    "        range = [Device]\n",
    "\n",
    "    # Annotation property for category (optional)\n",
    "    class category(AnnotationProperty):\n",
    "        pass\n",
    "\n",
    "    # Caches for reusing created classes\n",
    "    device_classes = {}             # Device classes\n",
    "    sensor_component_classes = {}   # SensorComponent classes, keyed by (device, sensor_category)\n",
    "    sensor_classes = {}             # Sensor model classes\n",
    "    part_classes = {}               # Part classes\n",
    "    property_classes = {}           # Property classes\n",
    "    technology_classes = {}         # Technology classes\n",
    "\n",
    "    # Process each sensor from the JSON data\n",
    "    for sensor_data in data[\"sensors\"]:\n",
    "        sensor_name_raw = sensor_data[\"name\"]\n",
    "        sensor_category_raw = sensor_data[\"category\"]\n",
    "        device_names = sensor_data.get(\"devices\", [])\n",
    "\n",
    "        # Clean and generate valid IRIs for sensor name and category\n",
    "        sensor_name = make_valid_iri_fragment(sensor_name_raw)\n",
    "        sensor_category = make_valid_iri_fragment(sensor_category_raw)\n",
    "        \n",
    "        # Skip if sensor name is empty\n",
    "        if not sensor_name:\n",
    "            print(f\"Skipping sensor with empty name.\")\n",
    "            continue\n",
    "\n",
    "        # Device-centric: Choose the first non-empty device from the list, or use a default.\n",
    "        if device_names and any(isinstance(d, str) and d.strip() for d in device_names):\n",
    "            for d in device_names:\n",
    "                if isinstance(d, list):\n",
    "                    d = \" \".join(d)\n",
    "                if isinstance(d, str) and d.strip():\n",
    "                    device_raw = d\n",
    "                    break\n",
    "        else:\n",
    "            device_raw = \"generic_device\"\n",
    "        \n",
    "        device_clean = make_valid_iri_fragment(device_raw)\n",
    "        if device_clean not in device_classes:\n",
    "            device_class = types.new_class(device_clean, (Device,))\n",
    "            device_classes[device_clean] = device_class\n",
    "        else:\n",
    "            device_class = device_classes[device_clean]\n",
    "\n",
    "        # Create a unique name for the sensor component: combine device and sensor category.\n",
    "        component_key = f\"{device_clean}_{sensor_category}\"\n",
    "        if component_key not in sensor_component_classes:\n",
    "            sensor_component = types.new_class(component_key, (SensorComponent, device_class))\n",
    "            sensor_component_classes[component_key] = sensor_component\n",
    "        else:\n",
    "            sensor_component = sensor_component_classes[component_key]\n",
    "\n",
    "        # Create the sensor model as a subclass of the sensor component.\n",
    "        sensor_class_name = sensor_name + \"_model\"\n",
    "        if sensor_class_name in sensor_classes:\n",
    "            sensor_class = sensor_classes[sensor_class_name]\n",
    "        else:\n",
    "            sensor_class = types.new_class(sensor_class_name, (sensor_component,))\n",
    "            sensor_classes[sensor_class_name] = sensor_class\n",
    "        \n",
    "        # (Optional) Add the category as an annotation\n",
    "        sensor_class.category.append(sensor_category)\n",
    "        \n",
    "        # Process parts\n",
    "        for part_name in sensor_data.get(\"parts\", []):\n",
    "            part_clean = make_valid_iri_fragment(part_name)\n",
    "            if part_clean not in part_classes:\n",
    "                part_class = types.new_class(part_clean, (Part,))\n",
    "                part_classes[part_clean] = part_class\n",
    "            else:\n",
    "                part_class = part_classes[part_clean]\n",
    "            sensor_class.is_a.append(has_part_directly.some(part_class))\n",
    "        \n",
    "        # Process properties\n",
    "        for prop_name in sensor_data.get(\"properties\", []):\n",
    "            prop_clean = make_valid_iri_fragment(prop_name)\n",
    "            if prop_clean not in property_classes:\n",
    "                prop_class = types.new_class(prop_clean, (Property,))\n",
    "                property_classes[prop_clean] = prop_class\n",
    "            else:\n",
    "                prop_class = property_classes[prop_clean]\n",
    "            sensor_class.is_a.append(has_property.some(prop_class))\n",
    "        \n",
    "        # Process technologies (implements)\n",
    "        for tech_name in sensor_data.get(\"implements\", []):\n",
    "            tech_clean = make_valid_iri_fragment(tech_name)\n",
    "            if tech_clean not in technology_classes:\n",
    "                tech_class = types.new_class(tech_clean, (Technology,))\n",
    "                technology_classes[tech_clean] = tech_class\n",
    "            else:\n",
    "                tech_class = technology_classes[tech_clean]\n",
    "            sensor_class.is_a.append(implements.some(tech_class))\n",
    "        \n",
    "        # Optionally, link sensor model directly to the device using \"used_in\"\n",
    "        sensor_class.is_a.append(used_in.some(device_class))\n",
    "\n",
    "\n",
    "\n",
    "# Save the ontology to a file\n",
    "onto.save(file=ontology_path, format=\"rdfxml\")\n",
    "\n",
    "print(\"Ontology created and saved as sensor_ontology_with_properties_and_implements.owl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzzy verification function using fuzzywuzzy\n",
    "def fuzzy_verify(name, chunks, threshold=75):\n",
    "    name_clean = name.lower()\n",
    "    max_score = 0\n",
    "    for chunk in chunks:\n",
    "        chunk_clean = chunk.page_content.lower()\n",
    "        score = fuzz.token_set_ratio(name_clean, chunk_clean)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "    return max_score, max_score >= threshold\n",
    "\n",
    "# Semantic verification using sentence-transformers\n",
    "def semantic_verify(sensor, chunks, model, threshold=0.65):\n",
    "    # Use sensor name and category for additional context\n",
    "    category = sensor.get('category', '').strip()\n",
    "    name = sensor.get('name', '').strip()\n",
    "    if category:\n",
    "        query_text = f\"LiDAR sensor model {name} used in {category}\"\n",
    "    else:\n",
    "        query_text = f\"LiDAR sensor model {name}\"\n",
    "        \n",
    "    query_embedding = model.encode(query_text)\n",
    "    \n",
    "    # Encode each chunk and compute cosine similarities\n",
    "    chunk_embeddings = [model.encode(chunk.page_content) for chunk in chunks]\n",
    "    similarities = cosine_similarity([query_embedding], chunk_embeddings)[0]\n",
    "    max_sim = float(np.max(similarities))\n",
    "    return max_sim, max_sim >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(generated_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    sensor_data = json.load(f)\n",
    "\n",
    "# Iterate through sensors and validate against the PDF chunks\n",
    "qa_results = []\n",
    "for sensor in sensor_data.get(\"sensors\", []):\n",
    "    name = sensor.get(\"name\", \"\")\n",
    "    \n",
    "    # Exact string search (case-insensitive)\n",
    "    string_found = any(name.lower() in chunk.lower() for chunk in chunks)\n",
    "    \n",
    "    # Fuzzy matching remains unchanged\n",
    "    fuzzy_score, fuzzy_match = fuzzy_verify(name, splits, threshold=80)\n",
    "    \n",
    "    # Use the updated semantic_verify function with contextual query text\n",
    "    semantic_score, semantic_match = semantic_verify(sensor, splits, entity_model, threshold=0.65)\n",
    "    \n",
    "    # Compute a weighted confidence score\n",
    "    confidence = (string_found * 0.4) + (fuzzy_score / 100 * 0.3) + (semantic_score * 0.3)\n",
    "    \n",
    "    qa_results.append({\n",
    "        \"sensor\": name,\n",
    "        \"string_match\": string_found,\n",
    "        \"fuzzy_score\": fuzzy_score,\n",
    "        \"fuzzy_match\": fuzzy_match,\n",
    "        \"semantic_score\": semantic_score,\n",
    "        \"semantic_match\": semantic_match,\n",
    "        \"confidence\": confidence\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(validation_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qa_results, f,indent=2)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
