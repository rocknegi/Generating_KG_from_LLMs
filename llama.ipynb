{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -Uqqq pip --progress-bar off\n",
    "%pip install -qqq ollama --progress-bar off\n",
    "%pip install -qqq pathlib --progress-bar off\n",
    "%pip install -qqq pandas --progress-bar off\n",
    "%pip install -qqq PyPDF2 --progress-bar off\n",
    "%pip install -qqq ollama --progress-bar off\n",
    "%pip install -qqq owlready2 --progress-bar off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from enum import Enum\n",
    "import PyPDF2\n",
    "import ollama\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from datetime import datetime\n",
    "import unicodedata\n",
    "\n",
    "MODEL = \"llama3.1:8b-instruct-q8_0\"\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text extracted from pdf\n"
     ]
    }
   ],
   "source": [
    "def extract_pdf_text(pdf_path):\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        text = \" \".join(page.extract_text() for page in reader.pages)\n",
    "    return text\n",
    "\n",
    "pdf_text = extract_pdf_text(\"sensors.pdf\")\n",
    "print(\"text extracted from pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseFormat(Enum):\n",
    "    JSON = \"json_object\"\n",
    "    TEXT = \"text\"\n",
    " \n",
    " \n",
    "def call_model(\n",
    "    prompt: str, response_format: ResponseFormat = ResponseFormat.TEXT\n",
    ") -> str:\n",
    "    response = ollama.generate(\n",
    "        model=MODEL,\n",
    "        prompt=prompt,\n",
    "        keep_alive=\"1h\",\n",
    "        format=\"\" if response_format == ResponseFormat.TEXT else \"json\",\n",
    "    )\n",
    "    return response[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SUMMARIZE_PROMPT = \"\"\"\n",
    "As a LiDAR sensor expert, your task is to extract, categorize, and group all the **real, specific, and branded** LiDAR sensor names mentioned in the following text.\n",
    "\n",
    "**Output Requirements**:\n",
    "1. Provide a concise and structured list of LiDAR sensors.\n",
    "2. Group and standardize similar categories to avoid duplicates (e.g., \"Automotive LiDAR\", \"Automotive LiDAR Sensors\", and \"Automotive Sensors\" should be grouped under one category, \"Automotive LiDAR\").\n",
    "3. Categorize the sensors based on their application, type, or any mentioned specifications (e.g., Automotive, Industrial, Surveying).\n",
    "4. Include only **specific branded or model names** of sensors (e.g., \"Velodyne Velarray\", \"Livox Horizon\"). \n",
    "\n",
    "**Exclusion Criteria**:\n",
    "- Do not include generic terms such as \"LiDAR\", \"LiDAR sensor\", \"LiDAR sensors\", \"various LiDAR sensors\", or \"no specific brand model mentioned\".\n",
    "- Exclude datasets or evaluation tools (e.g., \"Kitti\", \"NuScenes\").\n",
    "- Ignore descriptions or summaries like \"solid-state LiDAR\", \"lidar technology\", or \"selected sensors are mechanical or solid-state types\".\n",
    "- Avoid placeholder entries like \"not mentioned\" or \"various\".\n",
    "\n",
    "**Formatting**:\n",
    "- Strictly follow the JSON template below:\n",
    "{{\n",
    "    \"categories\": [\n",
    "        {{\n",
    "            \"name\": \"Category Name\",\n",
    "            \"sensors\": [\n",
    "                \"Sensor 1\",\n",
    "                \"Sensor 2\"\n",
    "            ]\n",
    "        }}\n",
    "    ],\n",
    "    \"metadata\": {{\n",
    "        \"total_categories\": <number>,\n",
    "        \"total_unique_sensors\": <number>\n",
    "    }}\n",
    "}}\n",
    "\n",
    "**Additional Instructions**:\n",
    "- Only include a category if it contains sensors.\n",
    "- Use consistent naming conventions for categories. Avoid redundancy or variations in names.\n",
    "- If no sensors are found in the text, return an empty JSON: {{}}.\n",
    "- Do not include any additional explanations or information.\n",
    "\n",
    "<text>\n",
    "{text}\n",
    "</text>\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks created and saved.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Chunk size for processing\n",
    "CHUNK_SIZE = 1000\n",
    "chunks = [pdf_text[i:i + CHUNK_SIZE] for i in range(0, len(pdf_text), CHUNK_SIZE)]\n",
    "\n",
    "# Save chunks to a file if needed\n",
    "with open(\"intermediate_data/chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunks, f)\n",
    "\n",
    "print(\"Chunks created and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model responses saved.\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Process chunks with the model and save responses\n",
    "responses = []\n",
    "\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    formatted_prompt = SUMMARIZE_PROMPT.format(text=chunk)\n",
    "    response = call_model(formatted_prompt)  # Call your expensive model\n",
    "    responses.append(response)\n",
    "\n",
    "    # Save responses after every chunk to ensure progress is retained\n",
    "    with open(\"intermediate_data/responses.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(responses, f)\n",
    "\n",
    "print(\"Model responses saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error parsing response: Here is the extracted list of real, specific, and branded LiDAR sensors in JSON format:\n",
      "\n",
      "{\n",
      "    \"categories\": [\n",
      "        {\n",
      "            \"name\": \"Automotive LiDAR\",\n",
      "            \"sensors\": [\n",
      "                \"Velodyne Velarray H800\",\n",
      "                \"Innoviz Pro\"\n",
      "            ]\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Industrial/Robotics LiDAR\",\n",
      "            \"sensors\": []\n",
      "        },\n",
      "        {\n",
      "            \"name\": \"Surveying LiDAR\",\n",
      "            \"sensors\": []\n",
      "        }\n",
      "    ],\n",
      "    \"metadata\": {\n",
      "        \"total_categories\": 3,\n",
      "        \"total_unique_sensors\": 6\n",
      "    }\n",
      "}\n",
      "\n",
      "Note that I have categorized the sensors into three categories: Automotive LiDAR, Industrial/Robotics LiDAR (since no specific application was mentioned but some sensors could be used in industrial or robotics settings), and Surveying LiDAR. However, since no sensors were specifically mentioned under Surveying LiDAR, this category remains empty.\n",
      "\n",
      "Here is the list of sensors without categories for easier reference:\n",
      "\n",
      "{\n",
      "    \"sensors\": [\n",
      "        \"Livox Horizon\",\n",
      "        \"Robosense M1\",\n",
      "        \"Blickfeld Cube\",\n",
      "        \"Blickfeld Cube Range\",\n",
      "        \"Velodyne Velarray H800\",\n",
      "        \"Innoviz Pro\"\n",
      "    ]\n",
      "}. Details: Expecting value: line 1 column 1 (char 0)\n",
      "Error parsing response: Here is the JSON output based on the text:\n",
      "\n",
      "{\n",
      "    \"categories\": [\n",
      "        {\n",
      "            \"name\": \"Automotive LiDAR\",\n",
      "            \"sensors\": [\n",
      "                \"Velodyne Velarray\",\n",
      "                \"Robosense M1\",\n",
      "                \"Livox Horizon\"\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"metadata\": {\n",
      "        \"total_categories\": 1,\n",
      "        \"total_unique_sensors\": 3\n",
      "    }\n",
      "}. Details: Expecting value: line 1 column 1 (char 0)\n",
      "Error parsing response: Here is the extracted list of LiDAR sensors in JSON format:\n",
      "\n",
      "{\n",
      "    \"categories\": [\n",
      "        {\n",
      "            \"name\": \"Automotive LiDAR\",\n",
      "            \"sensors\": [\n",
      "                \"Velodyne Velarray\",\n",
      "                \"Robosense M1\",\n",
      "                \"Livox Horizon\"\n",
      "            ]\n",
      "        }\n",
      "    ],\n",
      "    \"metadata\": {\n",
      "        \"total_categories\": 1,\n",
      "        \"total_unique_sensors\": 3\n",
      "    }\n",
      "}. Details: Expecting value: line 1 column 1 (char 0)\n",
      "Categories processed.\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Load responses and process them\n",
    "with open(\"intermediate_data/responses.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    responses = json.load(f)\n",
    "\n",
    "# Placeholder for final results\n",
    "categories_dict = defaultdict(set)\n",
    "\n",
    "# Function to clean strings\n",
    "def clean_string(s):\n",
    "    s = s.strip()  # Remove leading/trailing whitespace\n",
    "    s = s.encode('utf-8').decode('unicode_escape')  # Decode Unicode escapes\n",
    "    s = unicodedata.normalize('NFKC', s)  # Normalize special characters\n",
    "    s = re.sub(r\"[\\s\\-_/]+\", \" \", s)  # Replace special separators with space\n",
    "    s = s.replace(\"\\u00c2\", \"\")  # Remove unwanted artifacts\n",
    "    return s.lower()\n",
    "\n",
    "\n",
    "# Process responses into categories_dict\n",
    "for response in responses:\n",
    "    try:\n",
    "        response_json = json.loads(response)\n",
    "\n",
    "        if \"categories\" in response_json and isinstance(response_json[\"categories\"], list):\n",
    "            for category in response_json[\"categories\"]:\n",
    "                category_name = clean_string(category.get(\"name\", \"Unidentified Category\"))\n",
    "                sensors = {clean_string(sensor) for sensor in category.get(\"sensors\", [])}\n",
    "                categories_dict[category_name].update(sensors)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error parsing response: {response}. Details: {e}\")\n",
    "\n",
    "print(\"Categories processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to out/JSON/Extracted_Data_04.12.2024_131750.json\n"
     ]
    }
   ],
   "source": [
    "# Prepare the final JSON structure\n",
    "categories_list = []\n",
    "unique_sensors = set()\n",
    "\n",
    "# Merge categories and deduplicate sensors\n",
    "for category_name, sensors in categories_dict.items():\n",
    "    categories_list.append({\n",
    "        \"name\": category_name.title(),  # Capitalized name for better readability\n",
    "        \"sensors\": sorted(sensors)  # Sorted sensors\n",
    "    })\n",
    "    unique_sensors.update(sensors)\n",
    "\n",
    "output_json = {\n",
    "    \"categories\": categories_list,\n",
    "    \"metadata\": {\n",
    "        \"total_categories\": len(categories_list),\n",
    "        \"total_unique_sensors\": len(unique_sensors)\n",
    "    }\n",
    "}\n",
    "\n",
    "# Ensure that the final output strictly follows the required JSON template\n",
    "final_output = json.dumps(output_json, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Save the JSON to a file with a timestamp\n",
    "output_file = f\"out/JSON/Extracted_Data_{datetime.now().strftime('%d.%m.%Y_%H%M%S')}.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_output)\n",
    "\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from owlready2 import get_ontology, Thing, ObjectProperty\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the ontology URL\n",
    "ontology_url = \"http://example.org/lidar.owl\"\n",
    "\n",
    "# Function to create OWL ontology using triples\n",
    "def create_ontology_from_triples(json_data, ontology_url=ontology_url, output_file=\"lidar_ontology.owl\"):\n",
    "    \"\"\"\n",
    "    Convert JSON data to OWL ontology based on extracted triples and save it to a file.\n",
    "    \n",
    "    :param json_data: JSON dictionary containing the categorized sensor data.\n",
    "    :param ontology_url: The URL for the ontology namespace.\n",
    "    :param output_file: Filepath to save the resulting OWL file.\n",
    "    \"\"\"\n",
    "    # Load ontology\n",
    "    ontology = get_ontology(ontology_url)\n",
    "\n",
    "    with ontology:\n",
    "        # Define a base class for all categories\n",
    "        class LidarCategory(Thing):\n",
    "            pass\n",
    "        \n",
    "        # Define the object property 'has_sensor' to link categories with sensors\n",
    "        class has_sensor(ObjectProperty):\n",
    "            pass\n",
    "        \n",
    "        # Extract triples and populate ontology\n",
    "        for category in json_data.get(\"categories\", []):\n",
    "            category_name = category[\"name\"].replace(\" \", \"_\")  # Clean category name for OWL\n",
    "            category_class = type(category_name, (LidarCategory,), {})  # Create a class for the category\n",
    "\n",
    "            for sensor in category.get(\"sensors\", []):\n",
    "                sensor_name = sensor.replace(\" \", \"_\")  # Clean sensor name for OWL\n",
    "                # Create sensor instance\n",
    "                sensor_instance = category_class(sensor_name)\n",
    "                \n",
    "                # Create the triple (head: category, relation: has_sensor, tail: sensor_instance)\n",
    "                sensor_instance.has_sensor = [sensor_instance]  # Linking the sensor to the category\n",
    "\n",
    "    # Save the ontology to a file\n",
    "    ontology.save(file=output_file, format=\"rdfxml\")\n",
    "    print(f\"OWL file created at: {output_file}\")\n",
    "\n",
    "\n",
    "# Example JSON data as a string (same format as your example)\n",
    "json_data = json.loads(final_output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OWL file created at: out/OWL/LIDAR04.12.2024_144929.owl\n"
     ]
    }
   ],
   "source": [
    "json_data = json.loads(final_output)\n",
    "\n",
    "# Define output OWL file path\n",
    "output_owl_file = f\"out/OWL/LIDAR{datetime.now().strftime('%d.%m.%Y_%H%M%S')}.owl\"\n",
    "# Generate the OWL ontology\n",
    "create_ontology_from_json(json_data, output_file=output_owl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
