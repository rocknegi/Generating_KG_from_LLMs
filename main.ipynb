{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -Uqqq pip --progress-bar off\n",
    "# %pip install -qqq ollama --progress-bar off\n",
    "\n",
    "\n",
    "# %pip install -qqq owlready2 --progress-bar off\n",
    "\n",
    "# %pip install -qqq langchain-ollama --progress-bar off\n",
    "# %pip install -qqq langchain-community --progress-bar off\n",
    "# %pip install -qqq pypdf --progress-bar off\n",
    "\n",
    "# %pip install -qqq faiss-cpu --progress-bar off\n",
    "# %pip install -qqq rank_bm25 --progress-bar off\n",
    "# %pip install -qqq fuzzywuzzy --progress-bar off\n",
    "# %pip install -qqq scikit-learn --progress-bar off\n",
    "# %pip install -qqq sentence-transformers --progress-bar off\n",
    "# %pip install -qqq pandas --progress-bar off\n",
    "# %pip install -qqq matplotlib --progress-bar off\n",
    "# %pip install -qqq seaborn --progress-bar off\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "import ollama\n",
    "import json\n",
    "from enum import Enum\n",
    "import json\n",
    "import re\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.chat_models import ChatOllama\n",
    "from owlready2 import *\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from fuzzywuzzy import fuzz\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers import EnsembleRetriever, BM25Retriever, MultiQueryRetriever\n",
    "from langchain.embeddings import OllamaEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the SentenceTransformer model\n",
    "entity_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "MODEL = \"deepseek-r1:8b-llama-distill-q8_0\"\n",
    "\n",
    "file_path =\"PDFs/ImplantablePassiveSensors.pdf\"\n",
    "\n",
    "file_name = file_path.split('/')[-1].split('.')[0]\n",
    "response_path = 'responses/R1_temp_responses_{file}.json'.format(file=file_name)\n",
    "generated_path = 'generated_JSON/R1_temp_generated_{file}.json'.format(file=file_name)\n",
    "ontology_path = 'ontology/R1_temp_ontology_{file}.owl'.format(file=file_name)\n",
    "validation_json_path = 'QA/R1_temp_validation_{file}.json'.format(file=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove LaTeX equations\n",
    "    text = re.sub(r'\\$.*?\\$', '', text)  # Remove inline equations\n",
    "    # Fix hyphenated words\n",
    "    text = re.sub(r'(\\w+)-\\n(\\w+)', r'\\1\\2', text)\n",
    "    # Remove excessive whitespace\n",
    "    return re.sub(r'\\s+', ' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF\n",
    "pdf_loader = PyPDFLoader(file_path=file_path,extract_images=False)\n",
    "docs = pdf_loader.load();\n",
    "# Enhanced cleaning pipeline\n",
    "for doc in docs:\n",
    "    doc.page_content = clean_text(doc.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_splits(splits):\n",
    "    stats = {\n",
    "        'total_chunks': len(splits),\n",
    "        'avg_chunk_length': sum(len(c.page_content) for c in splits)/len(splits),\n",
    "        'max_length': max(len(c.page_content) for c in splits),\n",
    "        'min_length': min(len(c.page_content) for c in splits),\n",
    "        'metadata_fields': list(splits[0].metadata.keys()) if splits else []\n",
    "    }\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        length_function=lambda text: len(text.split()),  # Word-based counting\n",
    "        add_start_index=True,\n",
    "       separators=[\n",
    "        \"\\n\\n## \",    # Section headers\n",
    "        \"\\n\\n\",       # Paragraph breaks\n",
    "        \"\\n\",         # New lines\n",
    "        \"(?<!\\d)\\.(?!\\d)\\s+\",  # Sentence ends with space\n",
    "        \";\",          # Semi-colons\n",
    "        \", \",         # Commas\n",
    "        \" \"\n",
    "        ],\n",
    "        keep_separator=True,\n",
    "        is_separator_regex=True,\n",
    "    )\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "analyze_splits(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Store Embeddings in FAISS (Faster Retrieval)\n",
    "embeddings = OllamaEmbeddings(model=MODEL)\n",
    "faiss_store = FAISS.from_documents(splits, embeddings)\n",
    "\n",
    "# Step 2: Set Up Retrieval Methods\n",
    "vector_retriever = faiss_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "bm25_retriever = BM25Retriever.from_documents(splits)\n",
    "bm25_retriever.k = 3  # Match vector retriever\n",
    "\n",
    "# Dynamic Weights for Different Queries\n",
    "query_type = \"entity\"  # or \"relation\"\n",
    "weights = [0.8, 0.2] if query_type == \"entity\" else [0.6, 0.4]\n",
    "\n",
    "# Ensemble Retriever\n",
    "retriever = EnsembleRetriever(\n",
    "    retrievers=[vector_retriever, bm25_retriever],\n",
    "    weights=weights\n",
    ")\n",
    "llm = ChatOllama(model=MODEL) \n",
    "# Multi-Query Expansion (Better Recall)\n",
    "mq_retriever = MultiQueryRetriever.from_llm(retriever=retriever, llm=llm)\n",
    "\n",
    "# Retrieve Once, Then Filter\n",
    "combined_query = \"\"\"\n",
    "EXTRACT: Information about sensors, including their names or models, types \n",
    "(e.g., pressure, temperature, ultrasonic, optical, capacitive, MEMS),\n",
    "components, technical specifications (properties with units),\n",
    "and their applications or the devices they are part of.\n",
    "FIND: Key details describing sensor characteristics, measurement principles,\n",
    "functionality, design, and context of use.\n",
    "FOCUS: Sections explicitly discussing sensor technology, design, specifications,\n",
    "fabrication, or experimental setups involving specific sensors.\n",
    "IGNORE: General discussion without specific sensor details, citation markers,\n",
    "author names, affiliations.\n",
    "\"\"\"\n",
    "\n",
    "retrieved_docs = mq_retriever.get_relevant_documents(combined_query)\n",
    "\n",
    "retrieved_text = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with your local model\n",
    "compressor = LLMChainExtractor.from_llm(llm=llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever,\n",
    "    search_kwargs={\"k\": 10}  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KG_EXTRACTION_PROMPT = \"\"\"\n",
    "As an expert analyst specializing in extracting information about sensor technology from technical documents, \n",
    "analyze the provided text to identify and structure data about sensors.\n",
    "\n",
    "**Target Entities**:\n",
    "1. Sensor Name/Model: The specific name, model number, or descriptive identifier of the sensor \n",
    "  (e.g., CMUT pressure sensor, PT-1000, Velodyne VLP-16, F8-type sensor).\n",
    "   Extract the most specific identifier available.\n",
    "2. Category: The general type or classification of the sensor (e.g., Capacitive Pressure Sensor, \n",
    "   Temperature Sensor, LiDAR, Ultrasonic Transducer, MEMS Sensor, Optical Sensor).\n",
    "3. Components: Physical or functional parts of the sensor \n",
    "  (e.g., MEMS diaphragm, silicon substrate, TSV, laser diode, photodetector).\n",
    "4. Properties/Specifications: Quantifiable characteristics or performance metrics,\n",
    "   often with units (e.g., sensitivity: 34 fF/kPa, range: 0-10 Bar,\n",
    "   accuracy: ±0.01 Bar, resolution: 0.2°, wavelength: 905 nm, operating temperature: up to 350 °C). \n",
    "   Also include qualitative properties derived from context (e.g., comparative_sensitivity: > Sensor B).\n",
    "5. Implementations/Standards: Any specific standards, protocols, or notable techniques implemented or used by the \n",
    "    sensor (e.g., silicon-silicon bonding, TSV technology, ROS2, I2C).\n",
    "6. Device/Application: The larger system, device, or application area the sensor is used in \n",
    "    (e.g., Tire Pressure Monitoring System (TPMS), autonomous vehicles, industrial process control, \n",
    "    environmental monitoring).\n",
    "\n",
    "**Extraction Rules**:\n",
    "- Faithfully represent information stated in the text.\n",
    "- Preserve contextual relationships. Link components, properties, implementations, \n",
    "  and applications directly to the specific sensor they relate to.\n",
    "  *Example for Guidance Only (Do NOT include this data in your output)*: \n",
    "  \"The Model XYZ pressure sensor uses a MEMS diaphragm to detect changes up to 10 Bar\" -> \n",
    "  {{\"name\": \"Model XYZ\", \"category\": \"Pressure Sensor\", \"parts\": [\"MEMS diaphragm\"], \n",
    "    \"properties\": [\"pressure range: up to 10 Bar\"]}}\n",
    "- Capture implied properties from comparisons.\n",
    "  *Example for Guidance Only (Do NOT include this data in your output)*: \n",
    "  \"Sensor A is more sensitive than Sensor B\" -> {{\"properties\": [\"comparative_sensitivity: > Sensor B\"]}}\n",
    "- Identify the devices or applications associated with sensors.\n",
    "- Prioritize extracting specific, named sensors or clearly described sensor types. \n",
    "  Avoid vague references like \"a sensor\" unless no other identifier is available in the context.\n",
    "\n",
    "**Format**: Output ONLY the JSON object following this structure.\n",
    "{{\n",
    "  \"sensors\": [\n",
    "    {{\n",
    "      \"name\": \"\",\n",
    "      \"category\": \"\",\n",
    "      \"parts\": [],\n",
    "      \"properties\": [],\n",
    "      \"implements\": [],\n",
    "      \"device\": \"\"\n",
    "    }}\n",
    "    // Add more sensor objects if multiple are found\n",
    "  ]\n",
    "}}\n",
    "\n",
    "**Critical Instructions**:\n",
    "1. If no specific sensors or sensor types are clearly identified in the text, return: {{\"sensors\": []}}\n",
    "2. Only include sensor information that is explicitly mentioned or directly implied in the provided text.\n",
    "3. Do not invent information or include data from the guidance examples in your output.\n",
    "4. Ensure the output is ONLY the valid JSON object, with no additional text, commentary, or explanations before or \n",
    "   after it.\n",
    "5. Use exact names and values from the text whenever possible for properties and specifications.\n",
    "\n",
    "<text> {text} </text>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResponseFormat(Enum):\n",
    "    JSON = \"json_object\"\n",
    "    TEXT = \"text\"\n",
    " \n",
    " \n",
    "def call_model(\n",
    "    prompt: str, response_format: ResponseFormat = ResponseFormat.TEXT\n",
    ") -> str:\n",
    "    response = ollama.generate(\n",
    "        model=MODEL,\n",
    "        prompt=prompt,\n",
    "        keep_alive=\"1h\",\n",
    "        format=\"\" if response_format == ResponseFormat.TEXT else \"json\",\n",
    "    )\n",
    "    return response[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = text_splitter.split_text(retrieved_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the final prompt with the concatenated text\n",
    "\n",
    "responses = []\n",
    "\n",
    "for chunk in chunks:\n",
    "        final_prompt = KG_EXTRACTION_PROMPT.format(text=chunk)\n",
    "        # Send the final prompt to Ollama\n",
    "        # print (final_prompt)\n",
    "        response = call_model(final_prompt)\n",
    "        responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Save responses after every chunk to ensure progress is retained\n",
    "with open(response_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(responses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_string(s):\n",
    "    \"\"\"\n",
    "    Clean and normalize strings by stripping whitespace, decoding unicode escapes,\n",
    "    normalizing characters, replacing common separators with a space, and fixing known encoding issues.\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return str(s).lower()\n",
    "    s = s.strip()\n",
    "    s = unicodedata.normalize('NFKC', s)\n",
    "    s = s.replace(\"â\", \"\")\n",
    "    s = s.replace('<', ',')\n",
    "    s = s.replace('>', ',')\n",
    "    return s.lower() \n",
    "\n",
    "def make_valid_iri_fragment(s):\n",
    "    \"\"\"\n",
    "    Generate a valid IRI fragment from a string by first cleaning it and then\n",
    "    replacing spaces with underscores and removing disallowed characters.\n",
    "    \"\"\"\n",
    "    cleaned = clean_string(s)\n",
    "    # Replace spaces with underscores.\n",
    "    cleaned = cleaned.replace(\" \", \"_\")\n",
    "    # Remove any characters that are not alphanumerics, underscores, or hyphens.\n",
    "    cleaned = re.sub(r\"[^A-Za-z0-9_\\-]\", \"\", cleaned)\n",
    "    return cleaned\n",
    "\n",
    "\n",
    "def extract_json_part(response_str):\n",
    "    \"\"\"\n",
    "    Extract the JSON block from Ollama's response string.\n",
    "    Tries to find the JSON block delimited by ```json and ```.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        json_str = response_str.split(\"```json\")[-1].split(\"```\")[0].strip()\n",
    "\n",
    "        return json.loads(json_str)\n",
    "    except (IndexError, json.JSONDecodeError) as e:\n",
    "        print(f\"Failed to extract JSON: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Load responses from file\n",
    "with open(response_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_data = f.read()\n",
    "\n",
    "# Try parsing as a full JSON array; if that fails, split by lines.\n",
    "try:\n",
    "    responses = json.loads(raw_data)\n",
    "except json.JSONDecodeError:\n",
    "    responses = [json.loads(line) for line in raw_data.splitlines() if line.strip()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a dictionary keyed solely on sensor name\n",
    "sensor_dict = {}\n",
    "\n",
    "for response in responses:\n",
    "    # If the response is not a dict, try extracting the JSON block\n",
    "    if not isinstance(response, dict):\n",
    "        extracted = extract_json_part(response)\n",
    "        if extracted is None:\n",
    "            continue\n",
    "        response = extracted\n",
    "\n",
    "    if isinstance(response, list):\n",
    "        continue\n",
    "    sensors = response.get(\"sensors\", [])\n",
    "    if not isinstance(sensors, list):\n",
    "        continue\n",
    "\n",
    "    for sensor in sensors:\n",
    "        # Clean and extract fields from the sensor entry\n",
    "        sensor_name = clean_string(sensor.get(\"name\", \"unnamed sensor\"))\n",
    "        if not sensor_name:\n",
    "            continue\n",
    "        category_name = clean_string(sensor.get(\"category\", \"uncategorized\"))\n",
    "        parts_list = sensor.get(\"parts\", [])\n",
    "        properties_list = sensor.get(\"properties\", [])\n",
    "        implements_list = sensor.get(\"implements\", [])\n",
    "        # Note: if your field is \"devices\" (plural), adjust accordingly\n",
    "        device_info = clean_string(sensor.get(\"device\", \"\")) or \"\"\n",
    "        \n",
    "        cleaned_parts = set(clean_string(p) for p in parts_list if p)\n",
    "        cleaned_properties = set(clean_string(p) for p in properties_list if p)\n",
    "        cleaned_implements = set(clean_string(i) for i in implements_list if i)\n",
    "        devices = set()\n",
    "        if device_info:\n",
    "            devices.add(device_info)\n",
    "        \n",
    "        # Use sensor_name as the key for merging\n",
    "        key = sensor_name\n",
    "        \n",
    "        if key in sensor_dict:\n",
    "            sensor_dict[key][\"category\"].add(category_name)\n",
    "            sensor_dict[key][\"parts\"].update(cleaned_parts)\n",
    "            sensor_dict[key][\"properties\"].update(cleaned_properties)\n",
    "            sensor_dict[key][\"implements\"].update(cleaned_implements)\n",
    "            sensor_dict[key][\"devices\"].update(devices)\n",
    "        else:\n",
    "            sensor_dict[key] = {\n",
    "                \"name\": sensor_name,\n",
    "                \"category\": {category_name},\n",
    "                \"parts\": cleaned_parts,\n",
    "                \"properties\": cleaned_properties,\n",
    "                \"implements\": cleaned_implements,\n",
    "                \"devices\": devices\n",
    "            }\n",
    "\n",
    "# Convert sets to sorted lists and merge categories into a single string\n",
    "final_sensors = []\n",
    "for sensor in sensor_dict.values():\n",
    "    sensor[\"category\"] = \", \".join(sorted(sensor[\"category\"]))\n",
    "    sensor[\"parts\"] = sorted(sensor[\"parts\"])\n",
    "    sensor[\"properties\"] = sorted(sensor[\"properties\"])\n",
    "    sensor[\"implements\"] = sorted(sensor[\"implements\"])\n",
    "    sensor[\"devices\"] = sorted(sensor[\"devices\"])\n",
    "    final_sensors.append(sensor)\n",
    "\n",
    "# Save the deduplicated JSON\n",
    "with open(generated_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    json.dump({\"sensors\": final_sensors}, out_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Processed {len(final_sensors)} unique sensors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file\n",
    "with open(generated_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Create a new ontology\n",
    "onto = get_ontology(\"http://example.org/sensor_ontology.owl\")\n",
    "\n",
    "with onto:\n",
    "    # Define base classes\n",
    "    class Device(Thing):\n",
    "        pass\n",
    "\n",
    "    class SensorComponent(Thing):\n",
    "        pass\n",
    "\n",
    "    class Sensor(Thing):\n",
    "        pass\n",
    "    \n",
    "    class Part(Thing):\n",
    "        pass\n",
    "    \n",
    "    class Property(Thing):\n",
    "        pass\n",
    "    \n",
    "    class Technology(Thing):\n",
    "        pass\n",
    "\n",
    "    # Define object properties\n",
    "    class has_part_directly(ObjectProperty):\n",
    "        domain = [Sensor]\n",
    "        range = [Part]\n",
    "    \n",
    "    class has_property(ObjectProperty):\n",
    "        domain = [Sensor]\n",
    "        range = [Property]\n",
    "    \n",
    "    class implements(ObjectProperty):\n",
    "        domain = [Sensor]\n",
    "        range = [Technology]\n",
    "    \n",
    "    class used_in(ObjectProperty):\n",
    "        domain = [Sensor]\n",
    "        range = [Device]\n",
    "\n",
    "    class category(AnnotationProperty):\n",
    "        pass\n",
    "\n",
    "    # Caches for reusing created classes\n",
    "    device_classes = {}             # Device classes\n",
    "    sensor_component_classes = {}   # SensorComponent classes, keyed by (device, sensor_category)\n",
    "    sensor_classes = {}             # Sensor model classes\n",
    "    part_classes = {}               # Part classes\n",
    "    property_classes = {}           # Property classes\n",
    "    technology_classes = {}         # Technology classes\n",
    "\n",
    "    # Process each sensor from the JSON data\n",
    "    for sensor_data in data[\"sensors\"]:\n",
    "        sensor_name_raw = sensor_data[\"name\"]\n",
    "        sensor_category_raw = sensor_data[\"category\"]\n",
    "        device_names = sensor_data.get(\"devices\", [])\n",
    "\n",
    "        # Clean and generate valid IRIs for sensor name and category\n",
    "        sensor_name = make_valid_iri_fragment(sensor_name_raw)\n",
    "        sensor_category = make_valid_iri_fragment(sensor_category_raw)\n",
    "        \n",
    "        # Skip if sensor name is empty\n",
    "        if not sensor_name:\n",
    "            print(f\"Skipping sensor with empty name.\")\n",
    "            continue\n",
    "\n",
    "        # Device-centric: Choose the first non-empty device from the list, or use a default.\n",
    "        if device_names and any(isinstance(d, str) and d.strip() for d in device_names):\n",
    "            for d in device_names:\n",
    "                if isinstance(d, list):\n",
    "                    d = \" \".join(d)\n",
    "                if isinstance(d, str) and d.strip():\n",
    "                    device_raw = d\n",
    "                    break\n",
    "        else:\n",
    "            device_raw = \"generic_device\"\n",
    "        \n",
    "        device_clean = make_valid_iri_fragment(device_raw)\n",
    "        if device_clean not in device_classes:\n",
    "            device_class = types.new_class(device_clean, (Device,))\n",
    "            device_classes[device_clean] = device_class\n",
    "        else:\n",
    "            device_class = device_classes[device_clean]\n",
    "\n",
    "        # Create a unique name for the sensor component: combine device and sensor category.\n",
    "        component_key = f\"{device_clean}_{sensor_category}\"\n",
    "        if component_key not in sensor_component_classes:\n",
    "            sensor_component = types.new_class(component_key, (SensorComponent, device_class))\n",
    "            sensor_component_classes[component_key] = sensor_component\n",
    "        else:\n",
    "            sensor_component = sensor_component_classes[component_key]\n",
    "\n",
    "        # Create the sensor model as a subclass of the sensor component.\n",
    "        sensor_class_name = sensor_name + \"_model\"\n",
    "        if sensor_class_name in sensor_classes:\n",
    "            sensor_class = sensor_classes[sensor_class_name]\n",
    "        else:\n",
    "            sensor_class = types.new_class(sensor_class_name, (sensor_component,))\n",
    "            sensor_classes[sensor_class_name] = sensor_class\n",
    "        \n",
    "        # Add the category as an annotation\n",
    "        sensor_class.category.append(sensor_category)\n",
    "        \n",
    "        # Process parts\n",
    "        for part_name in sensor_data.get(\"parts\", []):\n",
    "            part_clean = make_valid_iri_fragment(part_name)\n",
    "            if part_clean not in part_classes:\n",
    "                part_class = types.new_class(part_clean, (Part,))\n",
    "                part_classes[part_clean] = part_class\n",
    "            else:\n",
    "                part_class = part_classes[part_clean]\n",
    "            sensor_class.is_a.append(has_part_directly.some(part_class))\n",
    "        \n",
    "        # Process properties\n",
    "        for prop_name in sensor_data.get(\"properties\", []):\n",
    "            prop_clean = make_valid_iri_fragment(prop_name)\n",
    "            if prop_clean not in property_classes:\n",
    "                prop_class = types.new_class(prop_clean, (Property,))\n",
    "                property_classes[prop_clean] = prop_class\n",
    "            else:\n",
    "                prop_class = property_classes[prop_clean]\n",
    "            sensor_class.is_a.append(has_property.some(prop_class))\n",
    "        \n",
    "        # Process technologies (implements)\n",
    "        for tech_name in sensor_data.get(\"implements\", []):\n",
    "            tech_clean = make_valid_iri_fragment(tech_name)\n",
    "            if tech_clean not in technology_classes:\n",
    "                tech_class = types.new_class(tech_clean, (Technology,))\n",
    "                technology_classes[tech_clean] = tech_class\n",
    "            else:\n",
    "                tech_class = technology_classes[tech_clean]\n",
    "            sensor_class.is_a.append(implements.some(tech_class))\n",
    "        \n",
    "        # Link sensor model directly to the device using \"used_in\"\n",
    "        sensor_class.is_a.append(used_in.some(device_class))\n",
    "\n",
    "\n",
    "\n",
    "# Save the ontology to a file\n",
    "onto.save(file=ontology_path, format=\"rdfxml\")\n",
    "\n",
    "print(\"Ontology created and saved as sensor_ontology_with_properties_and_implements.owl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fuzzy verification function using fuzzywuzzy\n",
    "def fuzzy_verify(name, chunks, threshold=70):\n",
    "\n",
    "    name_clean = name.lower()\n",
    "    max_score = 0\n",
    "    for chunk in chunks:\n",
    "        # Ensure we clean the chunk content consistently\n",
    "        chunk_clean = chunk.page_content.lower()\n",
    "        score = fuzz.token_set_ratio(name_clean, chunk_clean)\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "    return max_score, max_score >= threshold\n",
    "\n",
    "# Semantic verification using sentence-transformers\n",
    "def semantic_verify(sensor, chunks, model, threshold=0.65,top_n=3):\n",
    "    # Use sensor name and category for additional context\n",
    "    category = sensor.get('category', '').strip()\n",
    "    name = sensor.get('name', '').strip()\n",
    "    if category:\n",
    "        query_text = f\"Technical information about the sensor named {name} used in {category}\"\n",
    "    else:\n",
    "        query_text = f\"Technical information about the sensor named {name}\"\n",
    "        \n",
    "    query_embedding = model.encode(query_text)\n",
    "    \n",
    "    # Encode each chunk and compute cosine similarities\n",
    "    chunk_embeddings = [model.encode(chunk.page_content) for chunk in chunks]\n",
    "    similarities = cosine_similarity([query_embedding], chunk_embeddings)[0]\n",
    "    top_sim = np.mean(sorted(similarities, reverse=True)[:top_n])\n",
    "    return float(top_sim), top_sim >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(generated_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    sensor_data = json.load(f)\n",
    "\n",
    "# Iterate through sensors and validate against the PDF chunks\n",
    "qa_results = []\n",
    "for sensor in sensor_data.get(\"sensors\", []):\n",
    "    name = sensor.get(\"name\", \"\")\n",
    "    \n",
    "    # Exact string search (case-insensitive)\n",
    "    string_found = any(name.lower() in chunk.lower() for chunk in chunks)\n",
    "    \n",
    "    # Fuzzy matching remains unchanged\n",
    "    fuzzy_score, fuzzy_match = fuzzy_verify(name, splits, threshold=80)\n",
    "    \n",
    "    # Use the updated semantic_verify function with contextual query text\n",
    "    semantic_score, semantic_match = semantic_verify(sensor, splits, entity_model, threshold=0.65)\n",
    "    \n",
    "    # Compute a weighted confidence score\n",
    "    confidence = (string_found * 0.4) + (fuzzy_score / 100 * 0.3) + (semantic_score * 0.3)\n",
    "    \n",
    "    qa_results.append({\n",
    "        \"sensor\": name,\n",
    "        \"string_match\": bool(string_found),\n",
    "        \"fuzzy_score\": fuzzy_score,\n",
    "        \"fuzzy_match\": bool(fuzzy_match),\n",
    "        \"semantic_score\": semantic_score,\n",
    "        \"semantic_match\": bool(semantic_match),\n",
    "        \"confidence\": confidence\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(validation_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qa_results, f,indent=2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "with open(validation_json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    owl_data = json.load(f)\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(owl_data)\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Semantic Score vs. Confidence\n",
    "sns.scatterplot(ax=axes[0], data=df, x=\"semantic_score\", y=\"confidence\", hue=\"sensor\", palette=\"deep\")\n",
    "axes[0].set_title(\"Semantic Score vs. Confidence\")\n",
    "\n",
    "# Fuzzy Score vs. Confidence\n",
    "sns.scatterplot(ax=axes[1], data=df, x=\"fuzzy_score\", y=\"confidence\", hue=\"sensor\", palette=\"deep\")\n",
    "axes[1].set_title(\"Fuzzy Score vs. Confidence\")\n",
    "\n",
    "# Semantic Score vs. Fuzzy Score\n",
    "sns.scatterplot(ax=axes[2], data=df, x=\"semantic_score\", y=\"fuzzy_score\", hue=\"sensor\", palette=\"deep\")\n",
    "axes[2].set_title(\"Semantic Score vs. Fuzzy Score\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
